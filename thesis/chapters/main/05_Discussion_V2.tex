\chapter{Conclusion}\label{ch:discussion}

\section{Discussion, Interpretation, and Limitations}\label{sec:discussion}

The \textbf{research question} of this thesis included investigating the fairness of a machine learning model in mortgage lending and exploring the potential of explainability algorithms to detect and mitigate unfairness in the models:

\textit{“Can underlying unfairness in mortgage decision-making be detected, explained, and iteratively mitigated without sacrificing predictive performance?"}

The findings of this thesis suggest that the underlying unfairness in mortgage decision-making can be detected and partly explained using explainability algorithms. 
The analyses conducted supported the theory that the mortgage decisions according to the 2022 HMDA dataset favored \textbf{White} applicants (who also accounted for the majority of applications), leading to a neural network model trained on it being biased towards favoring \textbf{White} applicants over \textbf{Black or African American} applicants.
However, the iterative mitigation of this unfairness proved hard to achieve with the chosen algorithms. 
The fairness adjustments conducted in \textbf{chapter \ref{subsec:Iterations}} partly yielded improvements in fairness, but the models still favored \textbf{White} applicants over \textbf{Black or African American} applicants, even when the protected attribute itself was controlled for.
This assumption of a “deeper” inequality present in the data, resulting from \textbf{Black or African American} applicants historically being associated with worse attributes in terms of worthiness for a mortgage was supported by the analysis of the geographical enrichment data in \textbf{chapter \ref{subsec:Enrichment_Data}}, showing that the regions with a higher proportion of \textbf{Black or African American} applicants were correlated with lower chances of having a mortgage granted.

% Interpretation - Introductory Paragraph

% \textbf{Interpretation of Fairness and Explainability}
\textbf{Fairness and Explainability}

When comparing the findings of this thesis to the research conducted by Martinez and Kirchner \parencite{Martinez2021}, one common factor becomes apparent. Even though the differences in granted mortgages do not vary as extreme between the races analyzed (Martinez and Kircher report \textbf{Black or African American} applicants to have a 40\% - 80\% higher risk of having their applications denied), the factors influencing the decision-making process are similar.
Just as the findings of the explainability algorithms displayed in \textbf{chapter \ref{subsec:Explainability}} suggest, the \textit{debt to income ratio} appeared to be a major factor in the decision-making process, with \textbf{Black or African American} applicants having a higher risk of being denied a mortgage due to this factor.
While a direct comparison of the results is not possible due to the different datasets used and the different filters applied, the similarities in the factors influencing the decision-making process suggest that the underlying unfairness in mortgage decision-making is a systemic issue that is not easily mitigated.

When taking a historical perspective, the findings of this thesis are not surprising. The systemic discrimination against \textbf{Black or African American} applicants in mortgage lending has been well-documented in the past. 
Different studies on less recent versions of the HMDA data, such as those by Cherian (\cite{Cherian2014} for the years 1992 to 2003) or Cyree and Winters (\cite{Cyree2023} for the years 2007 to 2016) have shown that \textbf{Black or African American} applicants are systematically discriminated against in mortgage lending.
The findings of this thesis suggest that this discrimination is still present in the data used for the analysis, even though the data was collected in 2022, which supports the assumption that a big part of that discrimination is not "direct" but rather results from skin color being related to other detrimental decision factors, as has been assessed in \textbf{chapter \ref{subsec:Enrichment_Data}}.
It can be assumed that a more granular control for these factors might have provided a more detailed explanation of the unfairness present in the data. This is where studies like the one by Delis and Papadopoulos come in. They suggest that, even though discrimination cannot be ruled out for mortgage lending decisions, a tighter control of the omitted variables might show that race alone is not the deciding factor in the decision-making process \parencite{Delis2019}.

% \textbf{Interpretation of the EDA} % required in this form?



%\textbf{Interpretation of the Results}
\textbf{Results}

Both the performance (see \textbf{table \ref{tab:Model_Evaluation}}) and the subgroup performance (see \textbf{table \ref{tab:Fairness_Assessment_Initial}}) of the initial model surpass the performance of the algorithms promoted by Ghoba and Colaner \parencite{Ghoba}, which were used as a basis for the variable selection in this thesis. However, direct comparisons are impaired by the fact that no detailed methodology was provided in their paper.
Additionally, no feature importances have been assessed in this study, so a direct comparison of the results is not possible. Apart from the aforementioned study by Martinez and Kirchner, few papers report on feature importance in comparable setups. 
One of them is a pre-print by Alves et al., that suggests high feature importance of \textit{derived\_loan\_product\_type, intro\_rate\_period} (both of which have not been included in this thesis), as well as \textit{race} and \textit{gender} of the applicants, which have been included in this thesis but did not show high feature importance \parencite{alves:hal-03033181}.

% \textbf{Interpretation Summary}



\textbf{Limitations}

Limitations of the research conducted in this thesis can be found in the \textbf{data} used, the \textbf{models} applied, and the \textbf{fairness adjustments} conducted.

Although the \textbf{data} selection process was thorough and the data was cleaned and preprocessed following strict criteria, only focusing the analysis on five American states naturally limits the generalizability of the results for other geographies within America and internationally.
In terms of the features used it must be noted that with missingness in the \textit{debt\_to\_income\_ratio} appearing to be the most influential factor in the decision-making process, alternative approaches to handling missing data might have yielded different results.
Using more of the 99 features available in the HMDA dataset was not feasible from the standpoint of computational efficiency in this thesis but might have also yielded different results, as the assumed unfairness in the data might be more deeply rooted in other features not analyzed in this thesis.
Neither for the HMDA data, nor for the geographical enrichment data, causation can be assumed from the results presented in this thesis. The correlations found in the data might be due to other factors not analyzed in this thesis.

While the \textbf{model} choice of the neural network detailed in \textbf{table \ref{tab:CH03_Model_Details}} can be considered appropriate because of the widespread application of such models in practice, its generally good performance and it being a typical example of a \textit{black-box} algorithm, other models might have yielded different results.
Specifically, it has not been tested whether a less complex might have been able to achieve comparable results without the trade-off in inherent explainability.

The \textbf{fairness adjustments} conducted in this thesis were limited to the three iterations detailed in \textbf{chapter \ref{subsec:Iterations}}. While these adjustments were chosen based on their theoretical soundness, other fairness adjustments might have been appropriate for this task.
For example, no \textit{in-processing} algorithm was applied to keep the model used identical and therefore comparable across the iterations. This might have contributed to none of the fairness adjustments being able to fully mitigate the unfairness present in the data.
For the \textit{calibrated equalized odds} model, it must be noted that the authors themselves showed that enforcing calibration and equalized odds at the same time might impact performance in certain settings \parencite{Pleiss2017}, which can be assumed to have been the case in this thesis as well, given the comparably bad results of this adjustment.

Revisiting the research questions of this thesis (see \textbf{chapter \ref{ch:Introduction}}), it must be concluded that the initial aim of this work could only be achieved in parts. 
While the exploratory data analysis, coupled with the analysis of the geographical enrichment data was useful to \textit{detect} and partly \textit{explain} unfairness within the data, the \textit{mitigation} of this unfairness proved hard to achieve. 

\section{Recommendations and Conclusion}\label{sec:conclusion}

\textbf{Recommendations and Outlook}

These results yield important implications for the practical implementation of machine learning models, both in mortgage lending and generally. Not only must it be taken into account that automated decision-making in critical areas such as mortgage lending might learn from and amplify existing inequalities, but also that these inequalities might not be easily uncovered, even when attempting to explain the decision-making process of the models with common algorithmic techniques.
This implies that before the widespread application of machine learning models in mortgage lending (or, in fact, any are where the decision-making might have critical impact on individuals), a thorough analysis of the data used and the models applied must be conducted to ensure that the models do not amplify existing inequalities.

At the same time, the development of Artificial Intelligence progresses in a staggering and still increasing pace. 
By the time of writing this thesis, several multimodal Large Language Models (LLMs) are available to the broad public (examples being GPT-4o by OpenAI\footnote{https://openai.com/index/hello-gpt-4o/}, Google Gemini\footnote{https://deepmind.google/technologies/gemini/}, or the Microsoft Copilot\footnote{https://copilot.microsoft.com/}), while other promising model architectures like the Extended Long Short-Term Memory (xLSTM) are already challenging the transformer architecture \parencite{beck2024}.
These models are not only more powerful in terms of predictive performance but also in terms of their complexity and therefore their \textit{black-box} nature.
In times where such AI models are becoming an integral part of everyone's daily life to an extent that companies like OpenAI are already planning for the introduction of Artificial General Intelligence (AGI) \parencite{Altman2023}, while Google assumes that users will form 'deep relationships' with AI products \parencite{Harris2024}, it becomes very apparent that the need for explainability and fairness in AI models is dire due to their sheer impact.
It must be assumed that Artificial Intelligence will be applied to even more sensitive areas in the future (examples being healthcare, military, or the justice system). Consequences of unfairness in the models can be even more severe than in mortgage lending in these cases.
While some researchers are already tackling this issue by integrating explainability aspects into their work (a recent practical example being the CIMPLE project to tackle information manipulation\footnote{https://www.chistera.eu/projects/cimple}), additional research and practical application is required not only in the field of explainability but specifically also in its intersection with fairness considerations.

This is where this thesis aims to contribute to the field. By incorporating both explainability and fairness considerations into the model evaluation process, it aims not only to raise awareness that both concepts are important both in their own right but also in their intersection, while trying to provide a practical example of how these concepts can be applied in a real-world setting.
While the methodology of this thesis was focused specifically on unfairness in mortgage lending and does not provide a one-size-fits-all solution, the approach might be generalized to other areas after considering how to specifically adjust it to the given problem:
\begin{itemize}
    \item Is the problem one of binary classification or of a different nature?
    \item Are there subgroups that can be expected to be treated unfairly?
    \item Must the explainability algorithms be adjusted to the nature of the available data?
    \item Are the used fairness metrics feasible for the given problem?
\end{itemize}
For the data in this thesis, the procedure was feasible, because the data was binary, the subgroups were clearly defined and contained one (\textbf{Black or African American}) that could be expected to be systematically discriminated against, the explainability algorithms were applicable to the data, and the fairness metrics were chosen specifically for subgroup fairness. 
However, as already stated in the \textbf{limitations} considerations (see \textbf{section \ref{sec:discussion}}), the attempted fairness adjustments might not have been the most appropriate for the given problem.

While this thesis used a combination of different approaches to the topic of fairness, one way to truly improve the fairness of the models might be to include an even broader scope in terms of research focuses.
As Kim et al. \parencite{Kim2023} suggest in their review of fairness in credit, true multidisciplinary research is needed to tackle the issue of fairness in credit scoring.
But even within the narrower scope of this thesis, further research might be conducted to improve the fairness of the models applied. For example, the application of different fairness adjustments, the use of different models, or the inclusion of more features in the analysis might yield different results.
Other opportunities for future research close to the scope of this thesis might include the application of a different geographical scope, different target variables (e.g. the interest rate of the mortgage), or a stronger focus on causal inference to uncover the underlying reasons for the unfairness present in the data.
In a broader scope, a generalization of a methodology similar to the one applied in this thesis to other areas of machine learning might be a promising approach to improve the fairness of models in general.

\textbf{Concluding Summary}