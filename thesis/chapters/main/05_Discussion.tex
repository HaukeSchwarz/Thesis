\chapter{Conclusion}\label{ch:discussion}

\section{Discussion, Interpretation, and Limitations}\label{sec:discussion}

% 1: Restate Research Question

The \textbf{research question} of this thesis included investigating the fairness of a machine learning model in mortgage lending and exploring the potential of explainability algorithms to detect and mitigate unfairness in the models:

\textit{“Can underlying unfairness in mortgage decision-making be detected, explained, and iteratively mitigated without sacrificing predictive performance?"} %in a subset of the 2022 HMDA dataset?”}

% 2: Summarize Results

Indeed, the analyses conducted in this thesis supported the theory that the mortgage decisions according to the 2022 HMDA dataset favored \textbf{White} applicants (who also accounted for the majority of applications), leading to a neural network model trained on it being biased towards favoring \textbf{White} applicants over \textbf{Black or African American} applicants.
The explainability algorithms used in \textbf{chapter \ref{subsec:Explainability}} showed that the potentially discriminatory protected attributes were not a major influence on the model’s decision-making, but the patterns of the predicted data proved that the models did catch up on patterns within the data that promoted inequality.
This assumption of a “deeper” inequality present in the data, resulting from \textbf{Black or African American} applicants historically being associated with worse attributes in terms of worthiness for a mortgage was supported by the analysis of the geographical enrichment data in \textbf{chapter \ref{subsec:Enrichment_Data}}, showing that the regions with a higher proportion of \textbf{Black or African American} applicants were correlated with lower chances of having a mortgage granted.
The fairness adjustments conducted in \textbf{chapter \ref{subsec:Iterations}} partly yielded improvements in fairness, but the models still favored \textbf{White} applicants over \textbf{Black or African American} applicants, even when the protected attribute itself was controlled for.

% 3: Interpret Results (substructure according to results chapter)

% For a more thorough interpretation of the results, the individual chapters of this thesis will be revisited in the following paragraphs, starting with the benchmark model in \textbf{chapter \ref{subsec:Model_Training_and_Prediction}} and the explainability algorithms in \textbf{chapter \ref{subsec:Explainability}}.

% Results Chapter Structure:
%	Benchmark Model (incl. Fairness and Performance)
%	Explainability
%	Fairness adjustments (three iterations)

% \textbf{Benchmark Model}

The performance of the benchmark model (see \textbf{chapter \ref{tab:Model_Evaluation}}) was satisfactory with an accuracy of 90\%. However, as noted in \textbf{chapter \ref{subsec:mortgage_lending_fairness}}, finding directly comparable benchmarks proved difficult due to the narrowed down scope and filtering of the data in this thesis.

% XXX

% \textbf{Explainability}

Although the explainability algorithms used did not find the potentially discriminatory protected attributes to be a major influence on the model’s decision-making (compare \textbf{figure \ref{fig:SHAP_beeswarm}, \ref{fig:LIME_Individual_Analyses}}, and \textbf{\ref{fig:Global_Surrogate}}), the patterns of the predicted data prove that the models do catch up on patterns within the data that promote inequality. 
The fact that all models, even those specifically corrected for fairness in the sensitive \textit{applicant\_race-1\_White} mimic the fact present in the underlying data that \textbf{White} applicants have proportionally higher chances of having a mortgage granted (see \textbf{table \ref{tab:loan_granting}}) proves that. 
It must therefore be assumed that just as So et al. \parencite{So2022} suggested in their study, there is a “deeper” inequality present in the data, resulting from \textbf{Black or African American} applicants historically being associated with worse attributes in terms of worthiness for a mortgage, even when the protected attribute itself is controlled for.
Even when ignoring \textit{applicant\_race-1\_White} itself as an influencing factor of the decision-making process, the fact that historic inequalities lead to \textbf{Black or African American} applicants living for example in poorer or less educated geographies reintroduces discrimination into the analysis.
Geography as a factor potentially impacting fairness has been analyzed in \textbf{chapter \ref{subsec:HMDA_EDA}}. While no causation can be assumed from these results alone, it is clear that regions with a higher proportion of \textbf{Black or African American} applicants are correlated with lower chance of having a mortgage granted, hinting towards a potential discrimination based on this feature being present in the data.

When comparing the findings of this thesis to the research conducted by Martinez and Kircher \parencite{Martinez2021}, one common factor becomes apparent. Even though the differences in granted mortgages do not vary as extreme between the races analyzed (Martinez and Kircher report \textbf{Black or African American} applicants to have a 40\% - 80\% higher risk of having their applications denied), the factors influencing the decision-making process are similar.
Just as the findings of the explainability algorithms displayed in \textbf{chapter \ref{subsec:Explainability}} suggest, the \textit{debt to income ratio} appeared to be a major factor in the decision-making process, with \textbf{Black or African American} applicants having a higher risk of being denied a mortgage due to this factor.
While a direct comparison of the results is not possible due to the different datasets used and the different filters applied, the similarities in the factors influencing the decision-making process suggest that the underlying unfairness in mortgage decision-making is a systemic issue that is not easily mitigated.

% \textbf{Fairness Adjustments}



% \textbf{Limitations}

As will be explained in more detail in the following paragraphs, \textbf{limitations} of the research conducted in this thesis can be found in the \textbf{data} used, the \textbf{models} applied, and the \textbf{fairness adjustments} conducted.


Although the \textbf{data} selection process was thorough and the data was cleaned and preprocessed following strict criteria, only focusing the analysis on five American states naturally limits the generalizability of the results for other geographies within America and internationally.
In terms of the features used it must be noted that with missingness in the \textit{debt\_to\_income\_ratio} appearing to be the most influential factor in the decision-making process, alternative approaches to handling missing data might have yielded different results.
Using more of the 99 features available in the HMDA dataset was not feasible from the standpoint of computational efficiency in this thesis but might have also yielded different results, as the assumed unfairness in the data might be more deeply rooted in other features not analyzed in this thesis.
Neither for the HMDA data, nor for the geographical enrichment data, causation can be assumed from the results presented in this thesis. The correlations found in the data might be due to other factors not analyzed in this thesis.


While the \textbf{model} choice of the neural network detailed in \textbf{table \ref{tab:CH03_Model_Details}} can be considered appropriate because of the widespread application of such models in practice, its generally good performance and it being a typical example of a \textit{black-box} algorithm, other models might have yielded different results.
Specifically, it has not been tested whether a less complex might have been able to achieve comparable results without the trade-off in inherent explainability.


The \textbf{fairness adjustments} conducted in this thesis were limited to the three iterations detailed in \textbf{chapter \ref{subsec:Iterations}}. While these adjustments were chosen based on their theoretical soundness, other fairness adjustments might have been appropriate for this task.
For example, no \textit{in-processing} algorithm was applied to keep the model used identical and therefore comparable across the iterations. This might have contributed to none of the fairness adjustments being able to fully mitigate the unfairness present in the data.
For the \textit{calibrated equalized odds} model, it must be noted that the authors themselves showed that enforcing calibration and equalized odds at the same time might impact performance in certain settings \parencite{Pleiss2017}, which can be assumed to have been the case in this thesis as well, given the comparably bad results of this adjustment.


Revisiting the research questions of this thesis (see \textbf{chapter \ref{ch:Introduction}}), it must be concluded that the initial aim of this work could only be achieved in parts. 
While the exploratory data analysis, coupled with the analysis of the geographical enrichment data was useful to \textit{detect} and partly \textit{explain} unfairness within the data, the \textit{mitigation} of this unfairness proved hard to achieve. 
This might have been the case because of intrinsic unfairness being too deeply rooted within the data or the algorithms applied were not the optimal choices for this task.
Other possible causes of the dissatisfactory performance of the fairness adjustments might be the initial imbalance of the HMDA dataset with most of the applicants being white men (compare \textbf{chapter \ref{subsec:HMDA_EDA}}).

% Nevertheless, XXX


\section{Summary and Outlook}\label{sec:summary}

% 1: Summary

Concluding, this thesis added a new perspective to the ongoing discussion of algorithmic fairness by combining the concepts of \textit{fairness} and \textit{explainability} in the context of mortgage lending, leveraging a recent dataset.
The results of the analyses conducted in this thesis suggest that the underlying unfairness in mortgage decision-making can be detected and partly explained supported by explainability algorithms, but mitigating this unfairness without sacrificing predictive performance remains a challenge.
This ties with the findings of Ghoba and Colaner \parencite{Ghoba}, who conclude that perfect fairness in comparable prediction tasks might hardly be achievable by incremental changes to the model or the data.

% 2: Outlook + practical implementation

% XXX

These results yield important implications for the practical implementation of machine learning models in mortgage lending. Not only must it be taken into account that automated decision-making in critical areas such as mortgage lending might learn from and amplify existing inequalities, but also that these inequalities might not be easily uncovered, even when attempting to explain the decision-making process of the models with common algorithmic techniques.
This implies that before the widespread application of machine learning models in mortgage lending (or, in fact, any are where the decision-making might have critical impact on individuals), a thorough analysis of the data used and the models applied must be conducted to ensure that the models do not amplify existing inequalities.

While this thesis used a combination of different approaches to the topic of fairness, one way to truly improve the fairness of the models might be to include an even broader scope in terms of research focuses.
As Kim et al. \parencite{Kim2023} suggest in their review of fairness in credit, true multidisciplinary research is needed to tackle the issue of fairness in credit scoring.
But even within the narrower scope of this thesis, further research might be conducted to improve the fairness of the models applied. For example, the application of different fairness adjustments, the use of different models, or the inclusion of more features in the analysis might yield different results.
Other opportunities for future research might include the application of a different geographical scope, different target variables (e.g. the interest rate of the mortgage), or a stronger focus on causal inference to uncover the underlying reasons for the unfairness present in the data.