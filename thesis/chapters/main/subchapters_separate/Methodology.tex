\section{Methodology}\label{sec:Methodology}

The methodology used in this research is summarized in the flowchart in \textbf{figure \ref{fig:CH03_Methodology}}. It shows the three different main tasks that were focused on in this work:

\begin{enumerate}
    \item A Neural Network model aiming to predict mortgage approval was trained and tested on the HMDA mortgage dataset, serving as a \textbf{Benchmark} (see \textbf{chapter \ref{subsec:Model_Training_and_Prediction}}). Its performance was evaluated using a set of predefined metrics, \textit{metrics \#1}, and its fairness was assessed using a set of fairness metrics, \textit{metrics \#2} (see \textbf{chapter \ref{subsec:Model_Training_and_Prediction}})\footnote{All code produced for this thesis can be found at https://github.com/HaukeSchwarz/Thesis/tree/main/notebooks}. 
    \item Insights into the decision criteria of the benchmark model were gained through the application of three different \textbf{Explainability} algorithms: SHAP, LIME, and a Global Surrogate Model. Furthermore, geographical \textit{Enrichment Data} were utilized to further the understanding of which criteria may influence mortgage approval. For details on that step see \textbf{chapter \ref{subsec:Explainability}}.
    \item Finally, three different \textbf{Fairness Adjustments} algorithms were applied to the model or the data preparation process iteratively, each aiming to improve either \textit{metrics \#1} or \textit{metrics \#2}. These algorithms are described in \textbf{chapter \ref{subsec:Iterations}}.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{CH03_Methodology.jpg}
%    \caption{Methodology}
%    \caption*{The procedure chosen for this thesis follows an iterative approach: An initial model for classification of mortgage (dis)approval is trained and tested, serving as a \textit{Benchmark}. 
%    For this benchmark, fairness and performance are assessed based on a predefined set of metrics, \textit{metrics \#1} for model performance and \textit{metrics \#2} for model fairness. 
%    Additionally, different \textit{Explainability} techniques will be leveraged to gain insight into the decision criteria underlying the predictions of the benchmark model. 
%    Subsequently, three different \textit{Fairness Adjustments} are made to the model and the data preparation process iteratively, each aiming to improve one or more of the metrics. 
%    The results of each iteration are once again evaluated using the pre-defined metric sets.}
    \caption[Methodology]{\textbf{Methodology} - The procedure chosen for this thesis follows an iterative approach: An initial model for classification of mortgage (dis)approval is trained and tested, serving as a \textit{Benchmark}. 
    For this benchmark, fairness and performance are assessed based on a predefined set of metrics, \textit{metrics \#1} for model performance and \textit{metrics \#2} for model fairness. 
    Additionally, different \textit{Explainability} techniques will be leveraged to gain insight into the decision criteria underlying the predictions of the benchmark model. Subsequently, three different \textit{Fairness Adjustments} are made to the model and the data preparation process iteratively, each aiming to improve one or more of the metrics. 
    The results of each iteration are once again evaluated using the pre-defined metric sets.}
    \label{fig:CH03_Methodology}
\end{figure}

% Data Preparation and Splitting have already been discussed in \textbf{chapter \ref{subsec:Data_Preparation}}, details on the remaining steps will be provided in the following.

\subsection{Mortgage Classifier (Benchmark)}\label{subsec:Model_Training_and_Prediction}

Initially, a model aiming to predict mortgage approval as a classification task was set up. The model chosen comprised a comparably simple sequential neural network, implemented with the keras package\footnote{https://keras.io/about/}. Model details are provided in \textbf{table \ref{tab:CH03_Model_Details}}.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{lXr}
    \hline
    \textbf{Layer (type)} & \textbf{Output Shape} & \textbf{Param \#} \\
    \hline
    dense & (None, 32) & 640 \\
    dense\_1 & (None, 64) & 2112 \\
    dropout & (None, 64) & 0 \\
    dense\_2 & (None, 128) & 8320 \\
    dropout\_1 & (None, 128) & 0 \\
    dense\_3 & (None, 64) & 8256 \\
    dropout\_2 & (None, 64) & 0 \\
    dense\_4 & (None, 1) & 65 \\
    \hline
    \textbf{Total params} & & 19,393 \\
    \textbf{Trainable params} & & 19,393 \\
    \textbf{Non-trainable params} & & 0 \\
    \hline
    \end{tabularx}
%    \caption{Summary of the Neural Network}
%    \caption*{The neural network consisted of 5 dense layers (with 32, 64, 128, 64, and 1 neuron per layer), with 3 dropout layers (with dropout rates of 0.1, 0.25, and 0.1 respectively) in between. L2 regularization (0.001) is utilized in each dense layer. The total number of parameters is 19,393.}
    \medskip   
    \caption[Summary of the Neural Network]{\textbf{Summary of the Neural Network} - The neural network consisted of 5 dense layers (with 32, 64, 128, 64, and 1 neuron per layer), with 3 dropout layers (with dropout rates of 0.1, 0.25, and 0.1 respectively) in between. L2 regularization (0.001) is utilized in each dense layer. The total number of parameters is 19,393.}
    \label{tab:CH03_Model_Details}
\end{table}

To increase the efficiency of the training process and to prevent overfitting, \textit{callbacks} for early stopping (with a patience of 5 iterations) and best model selection, both based on the validation loss, have been implemented. 
\textit{Adam} was chosen as the optimizer, due to the nature of the classification task, loss evaluation was based on \textit{binary crossentropy}, and \textit{accuracy} was selected as the target metric. Training took place with a batch size of 48 and for a maximum of 30 epochs. 

\textbf{Performance Assessment (metrics \#1)}

The model originally put out numerical probabilities, which were then converted to binary values, using a threshold of 0.5.
Using these original probabilities, \textbf{ROC AUC} curves could be plotted and the ROC AUC score could be calculated, which was used as one of the assessment measures.
Additionally, a common set of classification metrics, being \textbf{accuracy}, \textbf{precision}, \textbf{recall}, and \textbf{F1 score} (see \textbf{equations \ref{eq:Accuracy}} to \textbf{\ref{eq:F1}}), are calculated alongside the \textbf{confusion matrix}.
This set of performance metrics is referred to as \textit{metrics \#1} in the methodology (see \textbf{figure \ref{fig:CH03_Methodology}}).

\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \label{eq:Accuracy}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}Accuracy}
\end{equation}

\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
    \label{eq:Precision}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}Precision}
\end{equation}

\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
    \label{eq:Recall}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}Recall}
\end{equation}

\begin{equation}
    F_1 = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
    \label{eq:F1}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}F1 Score}
\end{equation}

\textbf{Fairness Assessment (metrics \#2)}

The aequitas package provides a multitude of fairness metrics to provide model fairness. In order to work with a uniform framework in this thesis, four easy to understand yet highly relevant metrics are chosen: 
The disparities in \textbf{False Positive Rate} (FPR) and \textbf{False Negative Rate} (FNR), as well as the \textbf{True Positive Rate} (TPR) and \textbf{True Negative Rate} (TNR) are calculated for the different groups.
Analyzing these will inform about how much more likely the model is to make one of the four predictions for Black or African American Americans compared to White applicants.
From the original paper \parencite{2018aequitas}, the formulas for the calculation of the disparities can be inferred (see \textbf{equations \ref{eq:FPR_Disparity}} to \textbf{\ref{eq:TNR_Disparity}}):

\begin{equation}
    FPR_{g_{\text{disp}}} = \frac{FPR_{a_i}}{FPR_{a_r}} = \frac{\Pr(\hat{Y}=1 | Y=0, A=a_i)}{\Pr(\hat{Y}=1 | Y=0, A=a_r)}
    \label{eq:FPR_Disparity}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}FPR Disparity}
\end{equation}

\begin{equation}
    FNR_{g_{\text{disp}}} = \frac{FNR_{a_i}}{FNR_{a_r}} = \frac{\Pr(\hat{Y}=0 | Y=1, A=a_i)}{\Pr(\hat{Y}=0 | Y=1, A=a_r)}
    \label{eq:FNR_Disparity}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}FNR Disparity}
\end{equation}

\begin{equation}
    TPR_{g_{\text{disp}}} = \frac{TPR_{a_i}}{TPR_{a_r}} = \frac{\Pr(\hat{Y}=1 | Y=1, A=a_i)}{\Pr(\hat{Y}=1 | Y=1, A=a_r)}
    \label{eq:TPR_Disparity}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}TPR Disparity}
\end{equation}

\begin{equation}
    TNR_{g_{\text{disp}}} = \frac{TNR_{a_i}}{TNR_{a_r}} = \frac{\Pr(\hat{Y}=0 | Y=0, A=a_i)}{\Pr(\hat{Y}=0 | Y=0, A=a_r)}
    \label{eq:TNR_Disparity}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}TNR Disparity}
\end{equation}

Following the original paper, the optimal value for the disparities is \textbf{1}, as this would indicate parity between the groups. This means, that increased fairness is indicated by values closer to \textbf{1} when comparing model results to each other.
Analyzing these disparities result in the first set of fairness metrics assessed, denoted as \textit{metrics \#2} in the methodology (see \textbf{figure \ref{fig:CH03_Methodology}}).

The second part of \textit{metrics \#2} is a more specified analysis of the model performance for the different groups. If the model performed vastly different for the races in question, this would indicate a potential bias in the model. Therefore, race-specific performance metrics were analyzed.
More precisely, the outcomes were masked to only include either \textbf{White} or \textbf{Black or African American} applicants respectively, and the following metrics were calculated for each group:
\begin{itemize}
    \item \textbf{Accuracy}: The percentage of correct predictions made by the model.
    \item \textbf{Precision}: The percentage of correct positive predictions made by the model.
    \item \textbf{Recall}: The percentage of actual positive cases that were correctly predicted by the model.
    \item \textbf{F1 Score}: The harmonic mean of precision and recall.
    \item \textbf{AUC} (where applicable): The area under the ROC curve.
\end{itemize}

\subsection{Explainability}\label{subsec:Explainability}

As laid out in the research question for this thesis (see \textbf{chapter \ref{ch:Introduction}}), an important part of assessing the models fairness is the ability to actually explain its decisions.
A total of three different explainability algorithms (see \textbf{chapter \ref{subsec:algorithms}} for theoretical background) were utilized for two reasons: Firstly, to provide a comprehensive overview of the models' decision-making process, and secondly, to understand potential differences in the results of the algorithms, in case any may arise.
\begin{itemize}
    \item The \textbf{SHAP} algorithm was applied to a set of individual predictions, aiming to understand feature importance and the impact of individual features on the model's decision \parencite{Lundberg2017}.
    \item The \textbf{LIME} algorithm was used to challenge the predictions made by the SHAP algorithms to assess whether both provide comparable explanations \parencite{Ribeiro2016}.
    \item A \textbf{Global Surrogate Model} was trained on the model's prediction to assess whether the local explanations made by LIME and SHAP are also reflected on a global level (compare e.g. \cite{Molnar2023}).
\end{itemize}
The combination of these three algorithms aims to benefit not only model understanding, but also the analysis of potential differences in the predictions.

A separate approach to uncover factors that might influence the predictions of the model and therefore improve the ability to explain it is the use of geographical data (see \textbf{chapter \ref{subsec:Enrichment_Data}}). As the dataset contains information on the geographical location of the applicants, it was possible to analyze the distribution of granted loans across different regions by joining the mortgage data to economic variables of the corresponding regions using FIPS identifiers. 
While this procedure does not build upon the usage of an explainability algorithm per se, it does support the understanding of the model's decision-making process (thereby addressing both explainability and fairness concerns) by providing additional context that might not be directly grasped from the data itself.

In order to operationalize this approach, the following steps were taken:
\begin{itemize}
    \item The data was aggregated on \textit{county\_code} level.
    \item Aggregated KPIs were created: 
    \begin{itemize}
        \item \textbf{Sum of Applications}: The total number of applications per county, mainly important to filter out counties with a low number of applications.
        \item \textbf{Percentage of Grants}: The overall likelihood of a positive decision per county.
        \item \textbf{Percentage White Applicants}: The percentage of White applicants per county.
    \end{itemize}
    \item These KPIs were then related to the predictions made by the model by creating the \textbf{Perc. Pred. Grants} feature, which is the percentage of granted loans per county predicted by the model.
    \item Based on this set of information, several scatterplots relating these factors were created to analyze the distribution of granted loans across different regions, incorporating information on potentially discriminating factors. 
\end{itemize}

% \subsection{Performance Assessment (metrics \#1)}\label{subsec:Performance_Assessment}

% \subsection{Fairness Assessment (metrics \#2)}\label{subsec:Fairness_Assessment}

% \subsection{Explainability and Fairness through Enrichment Data}\label{subsec:Enrichment_Data_Methodology}

\subsection{Fairness Adjustments}\label{subsec:Iterations}

Based on the results of the performance and fairness assessments for the initial model run (see \textbf{chapter \ref{subsec:Iterations}}), adjustments to the model and the data preparation process were made in multiple iterations, aiming to improve at least one of these aspects in each run.

As a first iteration, \textbf{Reweighing} was applied. This \textit{pre-processing} procedure was initially developed by Calders et al. \parencite{Calders2009} and is implemented in the \textbf{AIF360} package\footnote{https://github.com/Trusted-AI/AIF360}. It works by adding a weight to each sample in the training data with the aim of balancing the weights of the different groups without actually adjusting any values.
The practical application of this technique includes the following steps: Initially, the weights of the samples need to be calculated. This can be achieved using the \textit{Reweighing} class of the \textit{aif360.algorithms.preprocessing} module. Being supplied with the training dataset and information on the privileged group (in this case, the \textit{White} applicants) and the unprivileged group (\textit{Black and African American} applicants), the algorithm calculates the weights for each sample. 
In order to ensure comparability of the results, an exact copy of the neural network described in \textbf{figure \ref{tab:CH03_Model_Details}} is created and compiled. During fitting however, the weights calculated by the reweighing algorithm are passed to the \textit{sample\_weight} parameter of the model, causing keras to apply them during model fitting. The results of this iteration can be found in \textbf{chapter \ref{subsec:Iterations}}.

The second iteration consisted of the application of the \textbf{Correlation Remover}, another pre-processing technique that was proposed by Weerts et al.\ in their paper on the Fairlearn Python package \parencite{Weerts2023}. It aims to remove any correlation between the sensitive attribute and the features of the dataset, while changes in non-sensitive features are kept as low as possible.
Practically implementing this technique requires a definition of the sensitive attribute $s$ (as opposed to non-sensitive attributes $z$), which will then be used to fulfill the constraint of the optimization problem as displayed in \textbf{equation \ref{eq:Correlation_Remover}}. 
Once again, the neural network needs to be retrained on the dataset after the application of the Correlation Remover. It must however be noted, that, as opposed to the reweighing technique, the underlying data are actually altered by this technique, meaning that a model fitted on correlation-removed data will only perform well on correlation-removed validation and testing data. The results of this iteration can be found in \textbf{chapter \ref{subsec:Iterations}}.

\begin{equation}
    \begin{aligned}
        & \underset{z_1, \ldots, z_n}{\text{min}}
        & & \sum_{i=1}^n ||z_i - x_i||^2 \\
        & \text{subject to}
        & & \frac{1}{n} \sum_{i=1}^n z_i (s_i - \bar{s})^T = 0
        \end{aligned}
    \label{eq:Correlation_Remover}
    \addcontentsline{frm}{formulas}{\protect\numberline{\theequation}\hspace{1em}Correlation Remover}
\end{equation}

As a third iteration, a post-processing technique was applied: The \textbf{Calibrated Equalized Odds Postprocessing} algorithm, initially proposed by Pleiss et al. \parencite{Pleiss2017}. It aims to satisfy the \textit{Equalized Odds} criterion (see \textbf{chapter \ref{subsec:overview}}) while keeping the results \textit{calibrated}, i.e. making sure that the predictions probabilities are interpretable as levels of confidence.
This algorithm has been implemented in the \textit{AIF360} package \footnote{More information on practical applications can be found under \url{https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_calibrated_eqodds_postprocessing.ipynb}} and can be applied to the model's predictions after the model has been trained by transforming the data with regards to the selected fairness constraint. The results of this iteration can be found in \textbf{chapter \ref{subsec:Iterations}}. 

% Add reasoning for each iteration!!!