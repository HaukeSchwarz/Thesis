@Article{SALEEM2022165,
  author   = {Rabia Saleem and Bo Yuan and Fatih Kurugollu and Ashiq Anjum and Lu Liu},
  journal  = {Neurocomputing},
  title    = {Explaining deep neural networks: A survey on the global interpretation methods},
  year     = {2022},
  issn     = {0925-2312},
  pages    = {165-180},
  volume   = {513},
  abstract = {A substantial amount of research has been carried out in Explainable Artificial Intelligence (XAI) models, especially in those which explain the deep architectures of neural networks. A number of XAI approaches have been proposed to achieve trust in Artificial Intelligence (AI) models as well as provide explainability of specific decisions made within these models. Among these approaches, global interpretation methods have emerged as the prominent methods of explainability because they have the strength to explain every feature and the structure of the model. This survey attempts to provide a comprehensive review of global interpretation methods that completely explain the behaviour of the AI models. We present a taxonomy of the available global interpretations models and systematically highlight the critical features and algorithms that differentiate them from local as well as hybrid models of explainability. Through examples and case studies from the literature, we evaluate the strengths and weaknesses of the global interpretation models and assess challenges when these methods are put into practice. We conclude the paper by providing the future directions of research in how the existing challenges in global interpretation methods could be addressed and what values and opportunities could be realized by the resolution of these challenges.},
  doi      = {https://doi.org/10.1016/j.neucom.2022.09.129},
  file     = {:C\:/Users/Hauke/OneDrive - ucp.pt/04_Thesis/01_Material/02_Other/Saleem et al._Explaining deep neural networks A survey on the global interpretation.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  keywords = {Artificial intelligence, Deep neural networks, Black box Models, Explainable artificial intelligence, Global interpretation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231222012218},
}

 
@Book{Ding2021,
  author     = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
  title      = {Retiring {Adult}: {New} {Datasets} for {Fair} {Machine} {Learning}},
  year       = {2021},
  month      = aug,
  abstract   = {Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions. Our datasets are available at https://github.com/zykls/folktables.},
  file       = {ResearchGate Link:https\://www.researchgate.net/publication/353838347_Retiring_Adult_New_Datasets_for_Fair_Machine_Learning:;:C\:/Users/Hauke/OneDrive - ucp.pt/04_Thesis/01_Material/02_Other/data/Ding et al_Retiring Adult New Datasets for Fair Machine Learning.pdf:PDF},
  groups     = {Data},
  shorttitle = {Retiring {Adult}},
}

 
@TechReport{Hall2020,
  author   = {Hall, Patrick},
  title    = {On the {Art} and {Science} of {Machine} {Learning} {Explanations}},
  year     = {2020},
  month    = may,
  note     = {arXiv:1810.02909 [cs, stat] type: article},
  abstract = {This text discusses several popular explanatory methods that go beyond the error measurements and plots traditionally used to assess machine learning models. Some of the explanatory methods are accepted tools of the trade while others are rigorously derived and backed by long-standing theory. The methods, decision tree surrogate models, individual conditional expectation (ICE) plots, local interpretable model-agnostic explanations (LIME), partial dependence plots, and Shapley explanations, vary in terms of scope, fidelity, and suitable application domain. Along with descriptions of these methods, this text presents real-world usage recommendations supported by a use case and public, in-depth software examples for reproducibility.},
  annote   = {Comment: This manuscript is a preprint of the text for an invited talk at the 2019 KDD XAI workshop. A previous version has also appeared in the proceedings of the Joint Statistical Meetings. Errata and updates available here: https://github.com/jphall663/kdd\_2019. Version 2 incorporated reviewer feedback. Version 3 includes a minor adjustment to Figure 1. Version 4 corrects a minor typo},
  doi      = {10.48550/arXiv.1810.02909},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1810.02909.pdf:application/pdf},
  groups   = {Overview xAI / Interpretability},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1810.02909},
  urldate  = {2024-02-13},
}

 
@Article{Marcinkevics2023,
  author     = {Marcinkevičs, Ričards and Vogt, Julia E.},
  journal    = {WIREs Data Mining and Knowledge Discovery},
  title      = {Interpretable and explainable machine learning: {A} methods-centric overview with concrete examples},
  year       = {2023},
  issn       = {1942-4795},
  number     = {3},
  pages      = {e1493},
  volume     = {13},
  abstract   = {Interpretability and explainability are crucial for machine learning (ML) and statistical applications in medicine, economics, law, and natural sciences and form an essential principle for ML model design and development. Although interpretability and explainability have escaped a precise and universal definition, many models and techniques motivated by these properties have been developed over the last 30 years, with the focus currently shifting toward deep learning. We will consider concrete examples of state-of-the-art, including specially tailored rule-based, sparse, and additive classification models, interpretable representation learning, and methods for explaining black-box models post hoc. The discussion will emphasize the need for and relevance of interpretability and explainability, the divide between them, and the inductive biases behind the presented “zoo” of interpretable models and explanation methods. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textgreater} Explainable AI Technologies {\textgreater} Machine Learning Commercial, Legal, and Ethical Issues {\textgreater} Social Considerations},
  copyright  = {© 2023 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.},
  doi        = {10.1002/widm.1493},
  file       = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/widm.1493:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {explainability, interpretability, machine learning, neural networks},
  language   = {en},
  shorttitle = {Interpretable and explainable machine learning},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1493},
  urldate    = {2024-02-13},
}

 
@Article{Linardatos2021,
  author     = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  journal    = {Entropy},
  title      = {Explainable {AI}: {A} {Review} of {Machine} {Learning} {Interpretability} {Methods}},
  year       = {2021},
  issn       = {1099-4300},
  month      = jan,
  number     = {1},
  pages      = {18},
  volume     = {23},
  abstract   = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/e23010018},
  file       = {:Linardatos2021 - Explainable AI_ a Review of Machine Learning Interpretability Methods.pdf:PDF},
  groups     = {Overview xAI / Interpretability},
  keywords   = {xai, machine learning, explainability, interpretability, fairness, sensitivity, black-box},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Explainable {AI}},
  url        = {https://www.mdpi.com/1099-4300/23/1/18},
  urldate    = {2024-02-13},
}

@Book{Molnar2023,
  author   = {Molnar, Christoph},
  title    = {Interpretable {Machine} {Learning}},
  year     = {2023},
  abstract = {Machine learning has great potential for improving products, processes and research. But computers usually do not explain their predictions which is a barrier to the adoption of machine learning. This book is about making machine learning models and their decisions interpretable.

After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. The focus of the book is on model-agnostic methods for interpreting black box models such as feature importance and accumulated local effects, and explaining individual predictions with Shapley values and LIME. In addition, the book presents methods specific to deep neural networks.

All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project. Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.},
  groups   = {Overview xAI / Interpretability},
  url      = {https://christophm.github.io/interpretable-ml-book/},
}

 
@Article{Teodorescu2020,
  author    = {Teodorescu, Mike Horia and Morse, Lily and Awwad, Yazeed and Kane, Jerry},
  journal   = {Academy of Management Proceedings},
  title     = {A {Framework} for {Fairer} {Machine} {Learning} in {Organizations}},
  year      = {2020},
  issn      = {0065-0668},
  month     = aug,
  number    = {1},
  pages     = {16889},
  volume    = {2020},
  abstract  = {The use of machine learning in organizations presents a double-edged sword: machine learning tools reduce costs on otherwise repetitive, time-consuming tasks, yet run the risks of introducing systematic unfairness in organizational processes. Issues of behavioral ethics in machine learning implementations in organizations have not been thoroughly addressed in prior literature, as many of the necessary concepts are disparate across three literatures – ethics, machine learning, and management. Further, tradeoffs between fairness criteria in machine learning have not been addressed with regards to organizations. We move research forward by introducing an organizing framework for selecting and implementing fair algorithms in organizations."},
  doi       = {10.5465/AMBPP.2020.16889abstract},
  file      = {:C\:/Users/Hauke/OneDrive - ucp.pt/04_Thesis/01_Material/02_Other/Morse et al._A Framework for Fairer Machine Learning in Organizations.pdf:PDF},
  groups    = {Other},
  keywords  = {AOM Annual Meeting Proceedings 2020},
  publisher = {Academy of Management},
  url       = {https://journals.aom.org/doi/10.5465/AMBPP.2020.16889abstract},
  urldate   = {2024-02-13},
}

 
@TechReport{DoshiVelez2017,
  author   = {Doshi-Velez, Finale and Kim, Been},
  title    = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
  year     = {2017},
  month    = mar,
  note     = {arXiv:1702.08608 [cs, stat] type: article},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  doi      = {10.48550/arXiv.1702.08608},
  file     = {:DoshiVelez2017 - Towards a Rigorous Science of Interpretable Machine Learning.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1702.08608},
  urldate  = {2024-02-17},
}

 
@Article{Gunning2019,
  author    = {Gunning, David and Aha, David W.},
  journal   = {AI Magazine},
  title     = {{DARPA}'s {Explainable} {Artificial} {Intelligence} {Program}},
  year      = {2019},
  issn      = {2371-9621},
  number    = {2},
  pages     = {44--58},
  volume    = {40},
  abstract  = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA's explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems' explanations improve user understanding, user trust, and user task performance.},
  copyright = {© 2019 The Authors. AI Magazine published by John Wiley \& Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence},
  doi       = {10.1609/aimag.v40i2.2850},
  file      = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1609/aimag.v40i2.2850:application/pdf},
  groups    = {Overview xAI / Interpretability},
  language  = {en},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v40i2.2850},
  urldate   = {2024-02-17},
}

 
@Article{BarredoArrieta2020,
  author     = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  journal    = {Information Fusion},
  title      = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
  year       = {2020},
  issn       = {1566-2535},
  month      = jun,
  pages      = {82--115},
  volume     = {58},
  abstract   = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  doi        = {10.1016/j.inffus.2019.12.012},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S1566253519308103/pdfft?isDTMRedir=true&download=true:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
  shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
  url        = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  urldate    = {2024-02-17},
}

 
@Article{Carvalho2019,
  author     = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  journal    = {Electronics},
  title      = {Machine {Learning} {Interpretability}: {A} {Survey} on {Methods} and {Metrics}},
  year       = {2019},
  issn       = {2079-9292},
  month      = aug,
  number     = {8},
  pages      = {832},
  volume     = {8},
  abstract   = {Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/electronics8080832},
  file       = {:Carvalho2019 - Machine Learning Interpretability_ a Survey on Methods and Metrics.pdf:PDF},
  groups     = {Overview xAI / Interpretability},
  keywords   = {machine learning, interpretability, explainability, XAI},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Machine {Learning} {Interpretability}},
  url        = {https://www.mdpi.com/2079-9292/8/8/832},
  urldate    = {2024-02-17},
}

 
@Article{Guidotti2018,
  author   = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  journal  = {ACM Computing Surveys},
  title    = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
  year     = {2018},
  issn     = {0360-0300},
  month    = aug,
  number   = {5},
  pages    = {93:1--93:42},
  volume   = {51},
  abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  doi      = {10.1145/3236009},
  file     = {:Guidotti2018 - A Survey of Methods for Explaining Black Box Models.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  keywords = {Open the black box, explanations, interpretability, transparent models},
  url      = {https://arxiv.org/abs/1802.01933},
  urldate  = {2024-02-18},
}

 
@InProceedings{Caruana2015,
  author     = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  booktitle  = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  title      = {Intelligible {Models} for {HealthCare}: {Predicting} {Pneumonia} {Risk} and {Hospital} 30-day {Readmission}},
  year       = {2015},
  address    = {New York, NY, USA},
  month      = aug,
  pages      = {1721--1730},
  publisher  = {Association for Computing Machinery},
  series     = {{KDD} '15},
  abstract   = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
  doi        = {10.1145/2783258.2788613},
  groups     = {Other},
  isbn       = {9781450336642},
  keywords   = {additive models, classification, healthcare, intelligibility, interaction detection, logistic regression, risk prediction},
  shorttitle = {Intelligible {Models} for {HealthCare}},
  url        = {https://doi.org/10.1145/2783258.2788613},
  urldate    = {2024-02-18},
}

 
@Misc{,
  title    = {Regulation - 2016/679 - {EN} - gdpr - {EUR}-{Lex}},
  groups   = {Other},
  language = {en},
  url      = {https://eur-lex.europa.eu/eli/reg/2016/679/oj},
  urldate  = {2024-02-18},
}

 
@Article{Lipton2018,
  author     = {Lipton, Zachary C.},
  journal    = {Queue},
  title      = {The {Mythos} of {Model} {Interpretability}: {In} machine learning, the concept of interpretability is both important and slippery.},
  year       = {2018},
  issn       = {1542-7730},
  month      = jun,
  number     = {3},
  pages      = {31--57},
  volume     = {16},
  abstract   = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
  doi        = {10.1145/3236386.3241340},
  file       = {:Lipton2018 - The Mythos of Model Interpretability_ in Machine Learning, the Concept of Interpretability Is Both Important and Slippery..pdf:PDF},
  groups     = {Overview xAI / Interpretability},
  shorttitle = {The {Mythos} of {Model} {Interpretability}},
  url        = {https://arxiv.org/abs/1606.03490},
  urldate    = {2024-02-18},
}

@InProceedings{Kim2016,
  author    = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Examples are not enough, learn to criticize! Criticism for Interpretability},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  groups    = {Overview xAI / Interpretability},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
}

 
@Article{Miller2019,
  author     = {Miller, Tim},
  journal    = {Artificial Intelligence},
  title      = {Explanation in artificial intelligence: {Insights} from the social sciences},
  year       = {2019},
  issn       = {0004-3702},
  month      = feb,
  pages      = {1--38},
  volume     = {267},
  abstract   = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  doi        = {10.1016/j.artint.2018.07.007},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0004370218305988/pdfft?md5=ec6948d3f66efe5e57d1336a54d1604d&pid=1-s2.0-S0004370218305988-main.pdf&isDTMRedir=Y:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
  shorttitle = {Explanation in artificial intelligence},
  url        = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
  urldate    = {2024-02-18},
}

 
@Article{Adadi2018,
  author     = {Adadi, Amina and Berrada, Mohammed},
  journal    = {IEEE Access},
  title      = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
  year       = {2018},
  issn       = {2169-3536},
  pages      = {52138--52160},
  volume     = {6},
  abstract   = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  doi        = {10.1109/ACCESS.2018.2870052},
  file       = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/ielx7/6287639/8274985/08466590.pdf?tp=&arnumber=8466590&isnumber=8274985&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg0NjY1OTA=:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {Conferences, Machine learning, Market research, Prediction algorithms, Machine learning algorithms, Biological system modeling, Explainable artificial intelligence, interpretable machine learning, black-box models},
  shorttitle = {Peeking {Inside} the {Black}-{Box}},
  url        = {https://ieeexplore.ieee.org/document/8466590},
  urldate    = {2024-02-19},
}

 
@Article{Du2019,
  author   = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
  journal  = {Communications of the ACM},
  title    = {Techniques for interpretable machine learning},
  year     = {2019},
  issn     = {0001-0782},
  month    = dec,
  number   = {1},
  pages    = {68--77},
  volume   = {63},
  abstract = {Uncovering the mysterious ways machine learning models make decisions.},
  doi      = {10.1145/3359786},
  file     = {:Du2019 - Techniques for Interpretable Machine Learning.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  url      = {https://arxiv.org/abs/1808.00033},
  urldate  = {2024-02-19},
}

 
@Article{Murdoch2019,
  author    = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Definitions, methods, and applications in interpretable machine learning},
  year      = {2019},
  month     = oct,
  number    = {44},
  pages     = {22071--22080},
  volume    = {116},
  abstract  = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  doi       = {10.1073/pnas.1900654116},
  file      = {Full Text PDF:https\://www.pnas.org/doi/pdf/10.1073/pnas.1900654116:application/pdf},
  groups    = {Overview xAI / Interpretability},
  publisher = {Proceedings of the National Academy of Sciences},
  url       = {https://www.pnas.org/doi/full/10.1073/pnas.1900654116},
  urldate   = {2024-02-19},
}

 
@InProceedings{Dwork2012,
  author    = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
  title     = {Fairness through awareness},
  year      = {2012},
  address   = {New York, NY, USA},
  month     = jan,
  pages     = {214--226},
  publisher = {Association for Computing Machinery},
  series    = {{ITCS} '12},
  abstract  = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  doi       = {10.1145/2090236.2090255},
  groups    = {Overview xAI / Interpretability},
  isbn      = {9781450311151},
  url       = {https://arxiv.org/abs/1104.3913},
  urldate   = {2024-02-19},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Overview xAI / Interpretability\;0\;1\;0x0000ffff\;\;Files providing overviews of different interpretability / explainability methods\;;
1 StaticGroup:Data\;0\;1\;0xffff00ff\;\;Potential Datasets\;;
1 StaticGroup:Other\;0\;0\;0xff008fff\;\;Theoretical Frameworks etc.\;;
}
