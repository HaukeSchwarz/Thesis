@Article{SALEEM2022165,
  author   = {Rabia Saleem and Bo Yuan and Fatih Kurugollu and Ashiq Anjum and Lu Liu},
  journal  = {Neurocomputing},
  title    = {Explaining deep neural networks: A survey on the global interpretation methods},
  year     = {2022},
  issn     = {0925-2312},
  pages    = {165-180},
  volume   = {513},
  abstract = {A substantial amount of research has been carried out in Explainable Artificial Intelligence (XAI) models, especially in those which explain the deep architectures of neural networks. A number of XAI approaches have been proposed to achieve trust in Artificial Intelligence (AI) models as well as provide explainability of specific decisions made within these models. Among these approaches, global interpretation methods have emerged as the prominent methods of explainability because they have the strength to explain every feature and the structure of the model. This survey attempts to provide a comprehensive review of global interpretation methods that completely explain the behaviour of the AI models. We present a taxonomy of the available global interpretations models and systematically highlight the critical features and algorithms that differentiate them from local as well as hybrid models of explainability. Through examples and case studies from the literature, we evaluate the strengths and weaknesses of the global interpretation models and assess challenges when these methods are put into practice. We conclude the paper by providing the future directions of research in how the existing challenges in global interpretation methods could be addressed and what values and opportunities could be realized by the resolution of these challenges.},
  doi      = {https://doi.org/10.1016/j.neucom.2022.09.129},
  file     = {:C\:/Users/Hauke/OneDrive - ucp.pt/04_Thesis/01_Material/02_Other/Saleem et al._Explaining deep neural networks A survey on the global interpretation.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  keywords = {Artificial intelligence, Deep neural networks, Black box Models, Explainable artificial intelligence, Global interpretation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231222012218},
}

 
@Article{Lipton2018,
  author     = {Lipton, Zachary C.},
  journal    = {Queue},
  title      = {The {Mythos} of {Model} {Interpretability}: {In} machine learning, the concept of interpretability is both important and slippery.},
  year       = {2018},
  issn       = {1542-7730},
  month      = jun,
  number     = {3},
  pages      = {31--57},
  volume     = {16},
  abstract   = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
  doi        = {10.1145/3236386.3241340},
  file       = {:Lipton2018 - The Mythos of Model Interpretability_ in Machine Learning, the Concept of Interpretability Is Both Important and Slippery..pdf:PDF},
  groups     = {Overview xAI / Interpretability},
  shorttitle = {The {Mythos} of {Model} {Interpretability}},
  url        = {https://arxiv.org/abs/1606.03490},
  urldate    = {2024-02-18},
}

@InProceedings{Kim2016,
  author    = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Examples are not enough, learn to criticize! Criticism for Interpretability},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  groups    = {Overview xAI / Interpretability},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
}

 
@Article{Miller2019,
  author     = {Miller, Tim},
  journal    = {Artificial Intelligence},
  title      = {Explanation in artificial intelligence: {Insights} from the social sciences},
  year       = {2019},
  issn       = {0004-3702},
  month      = feb,
  pages      = {1--38},
  volume     = {267},
  abstract   = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  doi        = {10.1016/j.artint.2018.07.007},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0004370218305988/pdfft?md5=ec6948d3f66efe5e57d1336a54d1604d&pid=1-s2.0-S0004370218305988-main.pdf&isDTMRedir=Y:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
  shorttitle = {Explanation in artificial intelligence},
  url        = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
  urldate    = {2024-02-18},
}

 
@Article{Du2019,
  author   = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
  journal  = {Communications of the ACM},
  title    = {Techniques for interpretable machine learning},
  year     = {2019},
  issn     = {0001-0782},
  month    = dec,
  number   = {1},
  pages    = {68--77},
  volume   = {63},
  abstract = {Uncovering the mysterious ways machine learning models make decisions.},
  doi      = {10.1145/3359786},
  file     = {:Du2019 - Techniques for Interpretable Machine Learning.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  url      = {https://arxiv.org/abs/1808.00033},
  urldate  = {2024-02-19},
}

 
@TechReport{Hoffman2019,
  author     = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  title      = {Metrics for {Explainable} {AI}: {Challenges} and {Prospects}},
  year       = {2019},
  month      = feb,
  note       = {arXiv:1812.04608 [cs] type: article},
  abstract   = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  doi        = {10.48550/arXiv.1812.04608},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1812.04608.pdf:application/pdf},
  groups     = {Benchmarking},
  keywords   = {Computer Science - Artificial Intelligence},
  school     = {arXiv},
  shorttitle = {Metrics for {Explainable} {AI}},
  url        = {http://arxiv.org/abs/1812.04608},
  urldate    = {2024-02-24},
}

 
@Article{Hall2021,
  author   = {Hall, Patrick and Cox, Benjamin and Dickerson, Steven and Ravi Kannan, Arjun and Kulkarni, Raghu and Schmidt, Nicholas},
  journal  = {Frontiers in Artificial Intelligence},
  title    = {A {United} {States} {Fair} {Lending} {Perspective} on {Machine} {Learning}},
  year     = {2021},
  issn     = {2624-8212},
  volume   = {4},
  abstract = {The use of machine learning (ML) has become more widespread in many areas of consumer financial services, including credit underwriting and pricing of loans. ML’s ability to automatically learn nonlinearities and interactions in training data is perceived to facilitate faster and more accurate credit decisions, and ML is now a viable challenger to traditional credit modeling methodologies. In this mini review, we further the discussion of ML in consumer finance by proposing uniform definitions of key ML and legal concepts related to discrimination and interpretability. We use the United States legal and regulatory environment as a foundation to add critical context to the broader discussion of relevant, substantial, and novel ML methodologies in credit underwriting, and we review numerous strategies to mitigate the many potential adverse implications of ML in consumer finance.},
  file     = {:Hall2021 - A United States Fair Lending Perspective on Machine Learning.pdf:PDF},
  groups   = {Fairness},
  url      = {https://www.frontiersin.org/articles/10.3389/frai.2021.695301},
  urldate  = {2024-02-25},
}

@Article{Zetten2022a,
  author        = {van Zetten, W. and Ramackers, G. J. and Hoos, H. H.},
  journal       = {Machine Learning with Applications},
  title         = {Increasing trust and fairness in machine learning applications within the mortgage industry},
  year          = {2022},
  issn          = {2666-8270},
  month         = dec,
  pages         = {100406},
  volume        = {10},
  abstract      = {The integration of machine learning in applications provides opportunities for increased efficiency in many organisations. However, the deployment of such systems is often hampered by the lack of insight into how their decisions are reached, resulting in concerns about trust and fairness. In this article, we investigate to what extent the addition of explainable AI components to ML applications can contribute to alleviating these issues. As part of this research, explainable AI functionality was developed for an existing ML model used for mortgage fraud detection at a large international financial institution based in The Netherlands A system implementing local explanation techniques was deployed to support the day-to-day work of fraud detection experts working with the model. In addition, a second system implementing global explanation techniques was developed to support the model management processes involving data-scientists, legal experts and compliance officers. A controlled experiment using actual mortgage applications was carried out to measure the effectiveness of these two systems, using both quantitative and qualitative assessment methods. Our results show that the addition of explainable AI functionality results in a statistically significant improvement in the levels of trust and usability by its daily users. The explainable AI system implementing global interpretability was found to considerably increase confidence in the ability to perform the processes focused on compliance and fairness. In particular, bias detection towards demographic groups successfully aided in the identification and removal of bias towards applicants with a migration background.},
  comment-hauke = {highly relevant both in terms of fairness and technicality},
  comment-hauke = {highly relevant both in terms of fairness and technicality},
  doi           = {10.1016/j.mlwa.2022.100406},
  file          = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S2666827022000810/pdfft?md5=3e2ded0fa4187136de8b9f8b05909034&pid=1-s2.0-S2666827022000810-main.pdf&isDTMRedir=Y:application/pdf},
  groups        = {Credit / Mortgage},
  keywords      = {Explainable artificial intelligence, Interpretable machine learning, Fair artificial intelligence, Bias detection, Fraud detection},
  url           = {https://www.sciencedirect.com/science/article/pii/S2666827022000810},
  urldate       = {2024-02-25},
}

 
@Article{Bhutta2022,
  author        = {Bhutta, Neil and Hizmo, Aurel and Ringo, Daniel},
  title         = {How {Much} {Does} {Racial} {Bias} {Affect} {Mortgage} {Lending}? {Evidence} from {Human} and {Algorithmic} {Credit} {Decisions}},
  year          = {2022},
  month         = oct,
  abstract      = {The Federal Reserve Board of Governors in Washington DC.},
  comment-hauke = {Good to quote for contribution and to use methodology},
  file          = {:Bhutta2022 - How Much Does Racial Bias Affect Mortgage Lending_ Evidence from Human and Algorithmic Credit Decisions.pdf:PDF},
  groups        = {Fairness},
  language      = {en},
  shorttitle    = {How {Much} {Does} {Racial} {Bias} {Affect} {Mortgage} {Lending}?},
  url           = {https://www.federalreserve.gov/econres/feds/how-much-does-racial-bias-affect-mortgage-lending.htm},
  urldate       = {2024-02-27},
}

 
@Article{Lee2021,
  author        = {Lee, Michelle Seng Ah and Floridi, Luciano},
  journal       = {Minds and Machines},
  title         = {Algorithmic {Fairness} in {Mortgage} {Lending}: from {Absolute} {Conditions} to {Relational} {Trade}-offs},
  year          = {2021},
  issn          = {1572-8641},
  month         = mar,
  number        = {1},
  pages         = {165--191},
  volume        = {31},
  abstract      = {To address the rising concern that algorithmic decision-making may reinforce discriminatory biases, researchers have proposed many notions of fairness and corresponding mathematical formalizations. Each of these notions is often presented as a one-size-fits-all, absolute condition; however, in reality, the practical and ethical trade-offs are unavoidable and more complex. We introduce a new approach that considers fairness—not as a binary, absolute mathematical condition—but rather, as a relational notion in comparison to alternative decisionmaking processes. Using US mortgage lending as an example use case, we discuss the ethical foundations of each definition of fairness and demonstrate that our proposed methodology more closely captures the ethical trade-offs of the decision-maker, as well as forcing a more explicit representation of which values and objectives are prioritised.},
  comment-hauke = {Same dataset, useful to justify approach},
  doi           = {10.1007/s11023-020-09529-4},
  file          = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs11023-020-09529-4.pdf:application/pdf},
  groups        = {Fairness},
  keywords      = {Algorithmic fairness, Mortgage discrimination, Fairness trade-offs, Machine learning, Technology ethics},
  language      = {en},
  shorttitle    = {Algorithmic {Fairness} in {Mortgage} {Lending}},
  url           = {https://doi.org/10.1007/s11023-020-09529-4},
  urldate       = {2024-02-27},
}

 
@misc{GDPR,
  title    = {Regulation - 2016/679 - {EN} - gdpr - {EUR}-{Lex}},
  groups   = {Other},
  language = {en},
  url      = {https://eur-lex.europa.eu/eli/reg/2016/679/oj},
  urldate  = {2024-02-18}
}

 
@book{Molnar2023,
  author   = {Molnar, Christoph},
  title    = {Interpretable {Machine} {Learning}},
  year     = {2023},
  abstract = {Machine learning has great potential for improving products, processes and research. But computers usually do not explain their predictions which is a barrier to the adoption of machine learning. This book is about making machine learning models and their decisions interpretable.
              
              After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. The focus of the book is on model-agnostic methods for interpreting black box models such as feature importance and accumulated local effects, and explaining individual predictions with Shapley values and LIME. In addition, the book presents methods specific to deep neural networks.
              
              All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project. Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.},
  groups   = {Overview xAI / Interpretability},
  url      = {https://christophm.github.io/interpretable-ml-book/}
}

@InProceedings{Choras2020,
  author    = {Chora{\'{s}}, Micha{\l} and Pawlicki, Marek and Puchalski, Damian and Kozik, Rafa{\l}},
  booktitle = {Computational Science -- ICCS 2020},
  title     = {Machine Learning -- The Results Are Not the only Thing that Matters! What About Security, Explainability and Fairness?},
  year      = {2020},
  address   = {Cham},
  editor    = {Krzhizhanovskaya, Valeria V. and Z{\'a}vodszky, G{\'a}bor and Lees, Michael H. and Dongarra, Jack J. and Sloot, Peter M. A. and Brissos, S{\'e}rgio and Teixeira, Jo{\~a}o},
  pages     = {615--628},
  publisher = {Springer International Publishing},
  abstract  = {Recent advances in machine learning (ML) and the surge in computational power have opened the way to the proliferation of ML and Artificial Intelligence (AI) in many domains and applications. Still, apart from achieving good accuracy and results, there are many challenges that need to be discussed in order to effectively apply ML algorithms in critical applications for the good of societies. The aspects that can hinder practical and trustful ML and AI are: lack of security of ML algorithms as well as lack of fairness and explainability. In this paper we discuss those aspects and provide current state of the art analysis of the relevant works in the mentioned domains.},
  groups    = {Fairness},
  isbn      = {978-3-030-50423-6},
}

@InBook{Zhou2022,
  author    = {Zhou, Jianlong and Chen, Fang and Holzinger, Andreas},
  editor    = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  pages     = {375--386},
  publisher = {Springer International Publishing},
  title     = {Towards Explainability for AI Fairness},
  year      = {2022},
  address   = {Cham},
  isbn      = {978-3-031-04083-2},
  abstract  = {AI explainability is becoming indispensable to allow users to gain insights into the AI system's decision-making process. Meanwhile, fairness is another rising concern that algorithmic predictions may be misaligned to the designer's intent or social expectations such as discrimination to specific groups. In this work, we provide a state-of-the-art overview on the relations between explanation and AI fairness and especially the roles of explanation on human's fairness judgement. The investigations demonstrate that fair decision making requires extensive contextual understanding, and AI explanations help identify potential variables that are driving the unfair outcomes. It is found that different types of AI explanations affect human's fairness judgements differently. Some properties of features and social science theories need to be considered in making senses of fairness with explanations. Different challenges are identified to make responsible AI for trustworthy decision making from the perspective of explainability and fairness.},
  booktitle = {xxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers},
  doi       = {10.1007/978-3-031-04083-2_18},
  groups    = {Fairness},
  url       = {https://doi.org/10.1007/978-3-031-04083-2_18},
}

@Conference{Deepak2021,
  author    = {Deepak Padmanabhan and Sanil V. and Joemon M. Jose},
  booktitle = {IJCAI 2021 Workshop on AI for Social Good},
  title     = {On Fairness and Interpretability},
  year      = {2021},
  abstract  = {Ethical AI spans a gamut of considerations. Among these, the most popular ones, fairness and interpretability, have remained largely distinct in technical pursuits. We discuss and elucidate the differences between fairness and interpretability across a variety of dimensions. Further, we develop two principles-based frameworks towards develop- ing ethical AI for the future that embrace aspects of both fairness and interpretability. First, interpretability for fairness proposes instantiating interpretability within the realm of fairness to develop a new breed of ethical AI. Second, fairness and interpretability initiates deliberations on bringing the best aspects of both together. We hope that these two frameworks will contribute to intensify- ing scholarly discussions on new frontiers of ethical AI that brings together fairness and interpretability.},
  groups    = {Fairness},
}

 
@TechReport{Hall2020,
  author   = {Hall, Patrick},
  title    = {On the {Art} and {Science} of {Machine} {Learning} {Explanations}},
  year     = {2020},
  month    = may,
  note     = {arXiv:1810.02909 [cs, stat] type: article},
  abstract = {This text discusses several popular explanatory methods that go beyond the error measurements and plots traditionally used to assess machine learning models. Some of the explanatory methods are accepted tools of the trade while others are rigorously derived and backed by long-standing theory. The methods, decision tree surrogate models, individual conditional expectation (ICE) plots, local interpretable model-agnostic explanations (LIME), partial dependence plots, and Shapley explanations, vary in terms of scope, fidelity, and suitable application domain. Along with descriptions of these methods, this text presents real-world usage recommendations supported by a use case and public, in-depth software examples for reproducibility.},
  annote   = {Comment: This manuscript is a preprint of the text for an invited talk at the 2019 KDD XAI workshop. A previous version has also appeared in the proceedings of the Joint Statistical Meetings. Errata and updates available here: https://github.com/jphall663/kdd\_2019. Version 2 incorporated reviewer feedback. Version 3 includes a minor adjustment to Figure 1. Version 4 corrects a minor typo},
  doi      = {10.48550/arXiv.1810.02909},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1810.02909.pdf:application/pdf},
  groups   = {Overview xAI / Interpretability},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1810.02909},
}

 
@Article{Marcinkevics2023,
  author     = {Marcinkevičs, Ričards and Vogt, Julia E.},
  journal    = {WIREs Data Mining and Knowledge Discovery},
  title      = {Interpretable and explainable machine learning: {A} methods-centric overview with concrete examples},
  year       = {2023},
  issn       = {1942-4795},
  number     = {3},
  pages      = {e1493},
  volume     = {13},
  abstract   = {Interpretability and explainability are crucial for machine learning (ML) and statistical applications in medicine, economics, law, and natural sciences and form an essential principle for ML model design and development. Although interpretability and explainability have escaped a precise and universal definition, many models and techniques motivated by these properties have been developed over the last 30 years, with the focus currently shifting toward deep learning. We will consider concrete examples of state-of-the-art, including specially tailored rule-based, sparse, and additive classification models, interpretable representation learning, and methods for explaining black-box models post hoc. The discussion will emphasize the need for and relevance of interpretability and explainability, the divide between them, and the inductive biases behind the presented “zoo” of interpretable models and explanation methods. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textgreater} Explainable AI Technologies {\textgreater} Machine Learning Commercial, Legal, and Ethical Issues {\textgreater} Social Considerations},
  copyright  = {© 2023 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.},
  doi        = {10.1002/widm.1493},
  file       = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/widm.1493:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {explainability, interpretability, machine learning, neural networks},
  language   = {en},
  shorttitle = {Interpretable and explainable machine learning},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1493},
}

 
@Article{Linardatos2021,
  author     = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  journal    = {Entropy},
  title      = {Explainable {AI}: {A} {Review} of {Machine} {Learning} {Interpretability} {Methods}},
  year       = {2021},
  issn       = {1099-4300},
  month      = jan,
  number     = {1},
  pages      = {18},
  volume     = {23},
  abstract   = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/e23010018},
  file       = {:Linardatos2021 - Explainable AI_ a Review of Machine Learning Interpretability Methods.pdf:PDF},
  groups     = {Overview xAI / Interpretability},
  keywords   = {xai, machine learning, explainability, interpretability, fairness, sensitivity, black-box},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Explainable {AI}},
  url        = {https://www.mdpi.com/1099-4300/23/1/18},
}

 
@Article{Teodorescu2020,
  author    = {Teodorescu, Mike Horia and Morse, Lily and Awwad, Yazeed and Kane, Jerry},
  journal   = {Academy of Management Proceedings},
  title     = {A {Framework} for {Fairer} {Machine} {Learning} in {Organizations}},
  year      = {2020},
  issn      = {0065-0668},
  month     = aug,
  number    = {1},
  pages     = {16889},
  volume    = {2020},
  abstract  = {The use of machine learning in organizations presents a double-edged sword: machine learning tools reduce costs on otherwise repetitive, time-consuming tasks, yet run the risks of introducing systematic unfairness in organizational processes. Issues of behavioral ethics in machine learning implementations in organizations have not been thoroughly addressed in prior literature, as many of the necessary concepts are disparate across three literatures – ethics, machine learning, and management. Further, tradeoffs between fairness criteria in machine learning have not been addressed with regards to organizations. We move research forward by introducing an organizing framework for selecting and implementing fair algorithms in organizations."},
  doi       = {10.5465/AMBPP.2020.16889abstract},
  file      = {:C\:/Users/Hauke/OneDrive - ucp.pt/04_Thesis/01_Material/02_Other/Morse et al._A Framework for Fairer Machine Learning in Organizations.pdf:PDF},
  groups    = {Other},
  keywords  = {AOM Annual Meeting Proceedings 2020},
  publisher = {Academy of Management},
  url       = {https://journals.aom.org/doi/10.5465/AMBPP.2020.16889abstract},
}

 
@TechReport{DoshiVelez2017,
  author   = {Doshi-Velez, Finale and Kim, Been},
  title    = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
  year     = {2017},
  month    = mar,
  note     = {arXiv:1702.08608 [cs, stat] type: article},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  doi      = {10.48550/arXiv.1702.08608},
  file     = {:DoshiVelez2017 - Towards a Rigorous Science of Interpretable Machine Learning.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1702.08608},
}

 
@Article{Gunning2019,
  author    = {Gunning, David and Aha, David W.},
  journal   = {AI Magazine},
  title     = {{DARPA}'s {Explainable} {Artificial} {Intelligence} {Program}},
  year      = {2019},
  issn      = {2371-9621},
  number    = {2},
  pages     = {44--58},
  volume    = {40},
  abstract  = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA's explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems' explanations improve user understanding, user trust, and user task performance.},
  copyright = {© 2019 The Authors. AI Magazine published by John Wiley \& Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence},
  doi       = {10.1609/aimag.v40i2.2850},
  file      = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1609/aimag.v40i2.2850:application/pdf},
  groups    = {Overview xAI / Interpretability},
  language  = {en},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v40i2.2850},
}

 
@Article{BarredoArrieta2020,
  author     = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  journal    = {Information Fusion},
  title      = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
  year       = {2020},
  issn       = {1566-2535},
  month      = jun,
  pages      = {82--115},
  volume     = {58},
  abstract   = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  doi        = {10.1016/j.inffus.2019.12.012},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S1566253519308103/pdfft?isDTMRedir=true&download=true:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
  shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
  url        = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
}

 
@Article{Carvalho2019,
  author     = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  journal    = {Electronics},
  title      = {Machine {Learning} {Interpretability}: {A} {Survey} on {Methods} and {Metrics}},
  year       = {2019},
  issn       = {2079-9292},
  month      = aug,
  number     = {8},
  pages      = {832},
  volume     = {8},
  abstract   = {Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/electronics8080832},
  file       = {:Carvalho2019 - Machine Learning Interpretability_ a Survey on Methods and Metrics.pdf:PDF},
  groups     = {Overview xAI / Interpretability},
  keywords   = {machine learning, interpretability, explainability, XAI},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Machine {Learning} {Interpretability}},
  url        = {https://www.mdpi.com/2079-9292/8/8/832},
}

 
@Article{Guidotti2018,
  author   = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  journal  = {ACM Computing Surveys},
  title    = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
  year     = {2018},
  issn     = {0360-0300},
  month    = aug,
  number   = {5},
  pages    = {93:1--93:42},
  volume   = {51},
  abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  doi      = {10.1145/3236009},
  file     = {:Guidotti2018 - A Survey of Methods for Explaining Black Box Models.pdf:PDF},
  groups   = {Overview xAI / Interpretability},
  keywords = {Open the black box, explanations, interpretability, transparent models},
  url      = {https://arxiv.org/abs/1802.01933},
}

 
@InProceedings{Caruana2015,
  author     = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  booktitle  = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  title      = {Intelligible {Models} for {HealthCare}: {Predicting} {Pneumonia} {Risk} and {Hospital} 30-day {Readmission}},
  year       = {2015},
  address    = {New York, NY, USA},
  month      = aug,
  pages      = {1721--1730},
  publisher  = {Association for Computing Machinery},
  series     = {{KDD} '15},
  abstract   = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
  doi        = {10.1145/2783258.2788613},
  groups     = {Other},
  isbn       = {9781450336642},
  keywords   = {additive models, classification, healthcare, intelligibility, interaction detection, logistic regression, risk prediction},
  shorttitle = {Intelligible {Models} for {HealthCare}},
  url        = {https://doi.org/10.1145/2783258.2788613},
}

 
@Article{Adadi2018,
  author     = {Adadi, Amina and Berrada, Mohammed},
  journal    = {IEEE Access},
  title      = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
  year       = {2018},
  issn       = {2169-3536},
  pages      = {52138--52160},
  volume     = {6},
  abstract   = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  doi        = {10.1109/ACCESS.2018.2870052},
  file       = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/ielx7/6287639/8274985/08466590.pdf?tp=&arnumber=8466590&isnumber=8274985&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg0NjY1OTA=:application/pdf},
  groups     = {Overview xAI / Interpretability},
  keywords   = {Conferences, Machine learning, Market research, Prediction algorithms, Machine learning algorithms, Biological system modeling, Explainable artificial intelligence, interpretable machine learning, black-box models},
  shorttitle = {Peeking {Inside} the {Black}-{Box}},
  url        = {https://ieeexplore.ieee.org/document/8466590},
}

 
@InProceedings{Dwork2012,
  author    = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
  title     = {Fairness through awareness},
  year      = {2012},
  address   = {New York, NY, USA},
  month     = jan,
  pages     = {214--226},
  publisher = {Association for Computing Machinery},
  series    = {{ITCS} '12},
  abstract  = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  doi       = {10.1145/2090236.2090255},
  groups    = {Overview xAI / Interpretability, Fairness},
  isbn      = {9781450311151},
  url       = {https://arxiv.org/abs/1104.3913},
}

 
@InProceedings{Le2023,
  author   = {Le, Phuong Quynh and Nauta, Meike and Nguyen, Van Bach and Pathak, Shreyasi and Schlötterer, Jörg and Seifert, Christin},
  title    = {Benchmarking {eXplainable} {AI} - {A} {Survey} on {Available} {Toolkits} and {Open} {Challenges}},
  year     = {2023},
  month    = aug,
  pages    = {6665--6673},
  volume   = {6},
  abstract = {Electronic proceedings of IJCAI 2023},
  comment  = {Comparing xAI Benchmarking Methods},
  doi      = {10.24963/ijcai.2023/747},
  file     = {:Le2023 - Benchmarking EXplainable AI a Survey on Available Toolkits and Open Challenges.pdf:PDF},
  groups   = {Benchmarking},
  issn     = {1045-0823},
  language = {en},
  url      = {https://www.ijcai.org/proceedings/2023/747},
}

 
@Article{Klosok2020,
  author    = {Kłosok, Marta and Chlebus, Marcin},
  journal   = {Working Papers},
  title     = {Towards better understanding of complex machine learning models using {Explainable} {Artificial} {Intelligence} ({XAI}) - case of {Credit} {Scoring} modelling},
  year      = {2020},
  abstract  = {recent years many scientific journals have widely explored the topic of machine learning interpretability. It is important as application of Artificial Intelligence is growing rapidly and its excellent performance is of huge potential for many. There is also need for overcoming the barriers faced by analysts implementing intelligent systems. The biggest one relates to the problem of explaining why the model made a certain prediction. This work brings the topic of methods for understanding a black-box from both the global and local perspective. Numerous agnostic methods aimed at interpreting black-box model behavior and predictions generated by these complex structures are analyzed. Among them are: Permutation Feature Importance, Partial Dependence Plot, Individual Conditional Expectation Curve, Accumulated Local Effects, techniques approximating predictions of the black-box for single observations with surrogate models (interpretable white-boxes) and Shapley values framework. Our prospect leads toward the question to what extent presented tools enhance model transparency. All of the frameworks are examined in practice with a credit default data use case. The overview presented prove that each of the method has some limitations, but overall almost all summarized techniques produce reliable explanations and contribute to higher transparency accountability of decision systems.},
  groups    = {Credit / Mortgage},
  keywords  = {machine learning, explainable Artificial Intelligence, visualization techniques, model interpretation, variable importance},
  language  = {en},
  publisher = {Faculty of Economic Sciences, University of Warsaw},
  url       = {https://www.wne.uw.edu.pl/files/9815/9355/2268/WNE_WP324.pdf},
}

 
@TechReport{Belaid2022,
  author        = {Belaid, Mohamed Karim and Hüllermeier, Eyke and Rabus, Maximilian and Krestel, Ralf},
  title         = {Do {We} {Need} {Another} {Explainable} {AI} {Method}? {Toward} {Unifying} {Post}-hoc {XAI} {Evaluation} {Methods} into an {Interactive} and {Multi}-dimensional {Benchmark}},
  year          = {2022},
  month         = oct,
  note          = {arXiv:2207.14160 [cs] type: article},
  abstract      = {In recent years, Explainable AI (xAI) attracted a lot of attention as various countries turned explanations into a legal right. xAI allows for improving models beyond the accuracy metric by, e.g., debugging the learned pattern and demystifying the AI's behavior. The widespread use of xAI brought new challenges. On the one hand, the number of published xAI algorithms underwent a boom, and it became difficult for practitioners to select the right tool. On the other hand, some experiments did highlight how easy data scientists could misuse xAI algorithms and misinterpret their results. To tackle the issue of comparing and correctly using feature importance xAI algorithms, we propose Compare-xAI, a benchmark that unifies all exclusive functional testing methods applied to xAI algorithms. We propose a selection protocol to shortlist non-redundant functional tests from the literature, i.e., each targeting a specific end-user requirement in explaining a model. The benchmark encapsulates the complexity of evaluating xAI methods into a hierarchical scoring of three levels, namely, targeting three end-user groups: researchers, practitioners, and laymen in xAI. The most detailed level provides one score per test. The second level regroups tests into five categories (fidelity, fragility, stability, simplicity, and stress tests). The last level is the aggregated comprehensibility score, which encapsulates the ease of correctly interpreting the algorithm's output in one easy to compare value. Compare-xAI's interactive user interface helps mitigate errors in interpreting xAI results by quickly listing the recommended xAI solutions for each ML task and their current limitations. The benchmark is made available at https://karim-53.github.io/cxai/},
  comment-hauke = {Overview of xAI Benchmarking Methods},
  doi           = {10.48550/arXiv.2207.14160},
  file          = {:Belaid2022 - Do We Need Another Explainable AI Method_ toward Unifying Post Hoc XAI Evaluation Methods into an Interactive and Multi Dimensional Benchmark.pdf:PDF},
  groups        = {Benchmarking},
  keywords      = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
  school        = {arXiv},
  shorttitle    = {Do {We} {Need} {Another} {Explainable} {AI} {Method}?},
  url           = {http://arxiv.org/abs/2207.14160},
}

 
@InProceedings{Demajo2020,
  author        = {Demajo, Lara Marie and Vella, Vince and Dingli, Alexiei},
  booktitle     = {Computer {Science} \& {Information} {Technology} ({CS} \& {IT})},
  title         = {Explainable {AI} for {Interpretable} {Credit} {Scoring}},
  year          = {2020},
  month         = nov,
  note          = {arXiv:2012.03749 [cs, q-fin]},
  pages         = {185--203},
  abstract      = {With the ever-growing achievements in Artificial Intelligence (AI) and the recent boosted enthusiasm in Financial Technology (FinTech), applications such as credit scoring have gained substantial academic interest. Credit scoring helps financial experts make better decisions regarding whether or not to accept a loan application, such that loans with a high probability of default are not accepted. Apart from the noisy and highly imbalanced data challenges faced by such credit scoring models, recent regulations such as the `right to explanation' introduced by the General Data Protection Regulation (GDPR) and the Equal Credit Opportunity Act (ECOA) have added the need for model interpretability to ensure that algorithmic decisions are understandable and coherent. An interesting concept that has been recently introduced is eXplainable AI (XAI), which focuses on making black-box models more interpretable. In this work, we present a credit scoring model that is both accurate and interpretable. For classification, state-of-the-art performance on the Home Equity Line of Credit (HELOC) and Lending Club (LC) Datasets is achieved using the Extreme Gradient Boosting (XGBoost) model. The model is then further enhanced with a 360-degree explanation framework, which provides different explanations (i.e. global, local feature-based and local instance-based) that are required by different people in different situations. Evaluation through the use of functionallygrounded, application-grounded and human-grounded analysis show that the explanations provided are simple, consistent as well as satisfy the six predetermined hypotheses testing for correctness, effectiveness, easy understanding, detail sufficiency and trustworthiness.},
  annote        = {Comment: 19 pages, David C. Wyld et al. (Eds): ACITY, DPPR, VLSI, WeST, DSA, CNDC, IoTE, AIAA, NLPTA - 2020},
  comment-hauke = {Quotable as approach to make credit scoring interpretable and as a source for related work},
  doi           = {10.5121/csit.2020.101516},
  file          = {:Demajo2020 - Explainable AI for Interpretable Credit Scoring.pdf:PDF},
  groups        = {Credit / Mortgage},
  keywords      = {Quantitative Finance - Risk Management, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  url           = {http://arxiv.org/abs/2012.03749},
}

 
@Article{Purificato2023,
  author    = {Purificato, Erasmo and Lorenzo, Flavio and Fallucchi, Francesca and De Luca, Ernesto William},
  journal   = {International Journal of Human–Computer Interaction},
  title     = {The {Use} of {Responsible} {Artificial} {Intelligence} {Techniques} in the {Context} of {Loan} {Approval} {Processes}},
  year      = {2023},
  issn      = {1044-7318},
  month     = apr,
  number    = {7},
  pages     = {1543--1562},
  volume    = {39},
  abstract  = {Despite the existing skepticism about the use of automatic systems in contexts where human knowledge and experience are considered indispensable (e.g., the granting of a mortgage, the prediction of stock prices, or the detection of cancers), our work aims to show how the use of explainability and fairness techniques can lead to the growth of a domain expert’s trust and reliance on an artificial intelligence (AI) system. This article presents a system, applied to the context of loan approval processes, focusing on the two aforementioned ethical principles out of the four defined by the High-Level Expert Group on AI in the document “Ethics Guidelines for Trustworthy AI,” published in April 2019, in which the key requirements that AI systems should meet to be considered trustworthy are identified. The presented case study is realized within a proprietary framework composed of several components for supporting the user throughout the management of the whole life cycle of a machine learning model. The main approaches, consisting of providing an interpretation of the model’s outputs and monitoring the model’s decisions to detect and react to unfair behaviors, are described in more detail to compare our system within state-of-the-art related frameworks. Finally, a novel Trust \& Reliance Scale is proposed for evaluating the system, and a usability test is performed to measure the user satisfaction with the effectiveness of the developed user interface; results are obtained, respectively, by the submission of the mentioned novel scale to bank domain experts and the usability questionnaire to a heterogeneous group composed of loan officers, data scientists, and researchers.},
  doi       = {10.1080/10447318.2022.2081284},
  file      = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/10447318.2022.2081284:application/pdf},
  groups    = {Credit / Mortgage},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/10447318.2022.2081284},
}

 
@TechReport{Misheva2021,
  author        = {Misheva, Branka Hadji and Osterrieder, Joerg and Hirsa, Ali and Kulkarni, Onkar and Lin, Stephen Fung},
  title         = {Explainable {AI} in {Credit} {Risk} {Management}},
  year          = {2021},
  month         = mar,
  note          = {arXiv:2103.00949 [cs, q-fin] type: article},
  abstract      = {Artificial Intelligence (AI) has created the single biggest technology revolution the world has ever seen. For the finance sector, it provides great opportunities to enhance customer experience, democratize financial services, ensure consumer protection and significantly improve risk management. While it is easier than ever to run state-of-the-art machine learning models, designing and implementing systems that support real-world finance applications have been challenging. In large part because they lack transparency and explainability which are important factors in establishing reliable technology and the research on this topic with a specific focus on applications in credit risk management. In this paper, we implement two advanced post-hoc model agnostic explainability techniques called Local Interpretable Model Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) to machine learning (ML)-based credit scoring models applied to the open-access data set offered by the US-based P2P Lending Platform, Lending Club. Specifically, we use LIME to explain instances locally and SHAP to get both local and global explanations. We discuss the results in detail and present multiple comparison scenarios by using various kernels available for explaining graphs generated using SHAP values. We also discuss the practical challenges associated with the implementation of these state-of-art eXplainabale AI (XAI) methods and document them for future reference. We have made an effort to document every technical aspect of this research, while at the same time providing a general summary of the conclusions.},
  comment-hauke = {Comparable paper with SHAP and LIME to explain credit scoring decisions},
  doi           = {10.48550/arXiv.2103.00949},
  file          = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2103.00949.pdf:application/pdf},
  groups        = {Credit / Mortgage},
  keywords      = {Quantitative Finance - Risk Management, Computer Science - Machine Learning},
  school        = {arXiv},
  url           = {http://arxiv.org/abs/2103.00949},
}

 
@Article{Bussmann2021,
  author        = {Bussmann, Niklas and Giudici, Paolo and Marinelli, Dimitri and Papenbrock, Jochen},
  journal       = {Computational Economics},
  title         = {Explainable {Machine} {Learning} in {Credit} {Risk} {Management}},
  year          = {2021},
  issn          = {1572-9974},
  month         = jan,
  number        = {1},
  pages         = {203--216},
  volume        = {57},
  abstract      = {The paper proposes an explainable Artificial Intelligence model that can be used in credit risk management and, in particular, in measuring the risks that arise when credit is borrowed employing peer to peer lending platforms. The model applies correlation networks to Shapley values so that Artificial Intelligence predictions are grouped according to the similarity in the underlying explanations. The empirical analysis of 15,000 small and medium companies asking for credit reveals that both risky and not risky borrowers can be grouped according to a set of similar financial characteristics, which can be employed to explain their credit score and, therefore, to predict their future behaviour.},
  comment-hauke = {Good example of using SHAP on credit scoring},
  doi           = {10.1007/s10614-020-10042-0},
  file          = {:Bussmann2021 - Explainable Machine Learning in Credit Risk Management.pdf:PDF},
  groups        = {Credit / Mortgage},
  keywords      = {Credit risk management, Explainable AI, Financial technologies, Similarity networks},
  language      = {en},
  url           = {https://doi.org/10.1007/s10614-020-10042-0},
}

 
@Article{Burrell2016,
  author     = {Burrell, Jenna},
  journal    = {Big Data \& Society},
  title      = {How the machine ‘thinks’: {Understanding} opacity in machine learning algorithms},
  year       = {2016},
  issn       = {2053-9517},
  month      = jun,
  number     = {1},
  volume     = {3},
  abstract   = {This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.},
  doi        = {10.1177/2053951715622512},
  file       = {SAGE PDF Full Text:https\://journals.sagepub.com/doi/pdf/10.1177/2053951715622512:application/pdf},
  groups     = {Overview xAI / Interpretability},
  publisher  = {SAGE Publications Ltd},
  shorttitle = {How the machine ‘thinks’},
  url        = {https://doi.org/10.1177/2053951715622512},
}

 
@Article{Murdoch2019,
  author    = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Definitions, methods, and applications in interpretable machine learning},
  year      = {2019},
  month     = oct,
  number    = {44},
  volume    = {116},
  abstract  = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  doi       = {10.1073/pnas.1900654116},
  file      = {Full Text PDF:https\://www.pnas.org/doi/pdf/10.1073/pnas.1900654116:application/pdf},
  groups    = {Overview xAI / Interpretability},
  publisher = {Proceedings of the National Academy of Sciences},
  url       = {https://www.pnas.org/doi/full/10.1073/pnas.1900654116},
}

 
@Article{Barocas2016,
  author    = {Barocas, Solon and Selbst, Andrew D.},
  journal   = {California Law Review},
  title     = {Big {Data}'s {Disparate} {Impact}},
  year      = {2016},
  issn      = {0008-1221},
  number    = {3},
  pages     = {671--732},
  volume    = {104},
  abstract  = {Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court. This Essay examines these concerns through the lens of American antidiscrimination law—more particularly, through Title VII's prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission's Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of protected groups, or flaws in the underlying data. Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of "discrimination" and "fairness."},
  file      = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/24758720.pdf?acceptTC=true:application/pdf},
  groups    = {Fairness},
  publisher = {California Law Review, Inc.},
  url       = {https://www.jstor.org/stable/24758720},
}

 
@TechReport{CorbettDavies2023,
  author   = {Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
  journal  = {Journal of Machine Learning Research},
  title    = {The {Measure} and {Mismeasure} of {Fairness}},
  year     = {2023},
  month    = aug,
  number   = {24},
  pages    = {1--117},
  note     = {arXiv:1808.00023 [cs] type: article},
  abstract = {The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.},
  doi      = {10.48550/arXiv.1808.00023},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1808.00023.pdf:application/pdf},
  groups   = {Fairness},
  keywords = {Computer Science - Computers and Society},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1808.00023},
}

 
@InCollection{Oneto2020,
  author    = {Oneto, Luca and Chiappa, Silvia},
  publisher = {Springer International Publishing},
  title     = {Fairness in {Machine} {Learning}},
  year      = {2020},
  address   = {Cham},
  editor    = {Oneto, Luca and Navarin, Nicolò and Sperduti, Alessandro and Anguita, Davide},
  isbn      = {9783030438838},
  pages     = {155--196},
  series    = {Studies in {Computational} {Intelligence}},
  abstract  = {Machine learning based systems are reaching society at large and in many aspects of everyday life. This phenomenon has been accompanied by concerns about the ethical issues that may arise from the adoption of these technologies. ML fairness is a recently established area of machine learning that studies how to ensure that biases in the data and model inaccuracies do not lead to models that treat individuals unfavorably on the basis of characteristics such as e.g. race, gender, disabilities, and sexual or political orientation. In this manuscript, we discuss some of the limitations present in the current reasoning about fairness and in methods that deal with it, and describe some work done by the authors to address them. More specifically, we show how causal Bayesian networks can play an important role to reason about and deal with fairness, especially in complex unfairness scenarios. We describe how optimal transport theory can be leveraged to develop methods that impose constraints on the full shapes of distributions corresponding to different sensitive attributes, overcoming the limitation of most approaches that approximate fairness desiderata by imposing constraints on the lower order moments or other functions of those distributions. We present a unified framework that encompasses methods that can deal with different settings and fairness criteria, and that enjoys strong theoretical guarantees. We introduce an approach to learn fair representations that can generalize to unseen tasks. Finally, we describe a technique that accounts for legal restrictions about the use of sensitive attributes.},
  doi       = {10.1007/978-3-030-43883-8_7},
  file      = {:Oneto2020 - Fairness in Machine Learning.html:URL},
  groups    = {Fairness},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-030-43883-8_7},
}

 
@TechReport{Chouldechova2018,
  author   = {Chouldechova, Alexandra and Roth, Aaron},
  title    = {The {Frontiers} of {Fairness} in {Machine} {Learning}},
  year     = {2018},
  month    = oct,
  note     = {arXiv:1810.08810 [cs, stat] type: article},
  abstract = {The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.},
  doi      = {10.48550/arXiv.1810.08810},
  file     = {:Chouldechova2018 - The Frontiers of Fairness in Machine Learning.pdf:PDF},
  groups   = {Fairness},
  keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computer Science and Game Theory, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1810.08810},
}

 
@InProceedings{Pradhan2022,
  author    = {Pradhan, Romila and Zhu, Jiongli and Glavic, Boris and Salimi, Babak},
  booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
  title     = {Interpretable {Data}-{Based} {Explanations} for {Fairness} {Debugging}},
  year      = {2022},
  address   = {New York, NY, USA},
  month     = jun,
  pages     = {247--261},
  publisher = {Association for Computing Machinery},
  series    = {{SIGMOD} '22},
  abstract  = {A wide variety of fairness metrics and eXplainable Artificial Intelligence (XAI) approaches have been proposed in the literature to identify bias in machine learning models that are used in critical real-life contexts. However, merely reporting on a model's bias or generating explanations using existing XAI techniques is insufficient to locate and eventually mitigate sources of bias. We introduce Gopher, a system that produces compact, interpretable, and causal explanations for bias or unexpected model behavior by identifying coherent subsets of the training data that are root-causes for this behavior. Specifically, we introduce the concept of causal responsibility that quantifies the extent to which intervening on training data by removing or updating subsets of it can resolve the bias. Building on this concept, we develop an efficient approach for generating the top-k patterns that explain model bias by utilizing techniques from the machine learning (ML) community to approximate causal responsibility, and using pruning rules to manage the large search space for patterns. Our experimental evaluation demonstrates the effectiveness of Gopher in generating interpretable explanations for identifying and debugging sources of bias.},
  doi       = {10.1145/3514221.3517886},
  file      = {:Pradhan2022 - Interpretable Data Based Explanations for Fairness Debugging.pdf:PDF},
  groups    = {Overview xAI / Interpretability, Fairness},
  isbn      = {9781450392495},
  keywords  = {data debugging, explanations, fairness, interpretability},
  url       = {https://dl.acm.org/doi/10.1145/3514221.3517886},
}

 
@TechReport{Datta2017,
  author   = {Datta, Anupam and Fredrikson, Matt and Ko, Gihyuk and Mardziel, Piotr and Sen, Shayak},
  title    = {Proxy {Non}-{Discrimination} in {Data}-{Driven} {Systems}},
  year     = {2017},
  month    = jul,
  note     = {arXiv:1707.08120 [cs] type: article},
  abstract = {Machine learnt systems inherit biases against protected classes, historically disparaged groups, from training data. Usually, these biases are not explicit, they rely on subtle correlations discovered by training algorithms, and are therefore difficult to detect. We formalize proxy discrimination in data-driven systems, a class of properties indicative of bias, as the presence of protected class correlates that have causal influence on the system's output. We evaluate an implementation on a corpus of social datasets, demonstrating how to validate systems against these properties and to repair violations where they occur.},
  annote   = {Comment: arXiv admin note: substantial text overlap with arXiv:1705.07807},
  doi      = {10.48550/arXiv.1707.08120},
  file     = {:Datta2017 - Proxy Non Discrimination in Data Driven Systems.pdf:PDF},
  groups   = {Fairness},
  keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1707.08120},
}

@InProceedings{Chen2019,
  author    = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  title     = {Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {339–348},
  publisher = {Association for Computing Machinery},
  series    = {FAT* '19},
  abstract  = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
  doi       = {10.1145/3287560.3287594},
  groups    = {Fairness},
  isbn      = {9781450361255},
  keywords  = {Bayesian Improved Surname Geocoding, disparate impact, fair lending, probablistic proxy model, protected class, race imputation, racial discrimination},
  location  = {Atlanta, GA, USA},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3287560.3287594},
}

 
@TechReport{Pessach2020,
  author   = {Pessach, Dana and Shmueli, Erez},
  title    = {Algorithmic {Fairness}},
  year     = {2020},
  month    = jan,
  note     = {arXiv:2001.09784 [cs, stat] type: article},
  abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.},
  annote   = {Comment: 31 pages, 1 figure, This is a survey article that reviews the field of algorithmic fairness},
  doi      = {10.48550/arXiv.2001.09784},
  file     = {:Pessach2020 - Algorithmic Fairness.pdf:PDF},
  groups   = {Fairness},
  keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2001.09784},
}

@Article{Bellamy2019,
  author  = {Bellamy, Rachel and Dey, Kuntal and Hind, Michael and Hoffman, Samuel and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Natesan Ramamurthy, Karthikeyan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush and Zhang, Yunfeng},
  journal = {IBM Journal of Research and Development},
  title   = {AI Fairness 360: An Extensible Toolkit for Detecting and Mitigating Algorithmic Bias},
  year    = {2019},
  month   = {09},
  pages   = {1-1},
  volume  = {PP},
  doi     = {10.1147/JRD.2019.2942287},
  groups  = {Fairness},
}

@InProceedings{Feldman2015,
  author    = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title     = {Certifying and Removing Disparate Impact},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {259–268},
  publisher = {Association for Computing Machinery},
  series    = {KDD '15},
  abstract  = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
  doi       = {10.1145/2783258.2783311},
  groups    = {Fairness},
  isbn      = {9781450336642},
  keywords  = {machine learning, fairness, disparate impact},
  location  = {Sydney, NSW, Australia},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2783258.2783311},
}

 
@Article{Mehrabi2021,
  author   = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal  = {ACM Computing Surveys},
  title    = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
  year     = {2021},
  issn     = {0360-0300},
  month    = jul,
  number   = {6},
  pages    = {115:1--115:35},
  volume   = {54},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  doi      = {10.1145/3457607},
  groups   = {Fairness},
  keywords = {Fairness and bias in artificial intelligence, deep learning, machine learning, natural language processing, representation learning},
  url      = {https://arxiv.org/abs/1908.09635},
}

 
@Article{Kamiran2012,
  author   = {Kamiran, Faisal and Calders, Toon},
  journal  = {Knowledge and Information Systems},
  title    = {Data preprocessing techniques for classification without discrimination},
  year     = {2012},
  issn     = {0219-3116},
  month    = oct,
  number   = {1},
  pages    = {1--33},
  volume   = {33},
  abstract = {Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.},
  doi      = {10.1007/s10115-011-0463-8},
  file     = {:Kamiran2012 - Data Preprocessing Techniques for Classification without Discrimination.pdf:PDF},
  groups   = {Fairness},
  keywords = {Classification, Preprocessing, Discrimination-aware data mining},
  language = {en},
  url      = {https://doi.org/10.1007/s10115-011-0463-8},
}

@InProceedings{Kamishima2012,
  author    = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  title     = {Fairness-Aware Classifier with Prejudice Remover Regularizer},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  editor    = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
  pages     = {35--50},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.},
  groups    = {Fairness},
  isbn      = {978-3-642-33486-3},
}


@Article{Alessandro2017,
  author  = {d'Alessandro, Brian and O'Neil, Cathy and LaGatta, Tom},
  journal = {Big Data},
  title   = {Conscientious Classification: A Data Scientist's Guide to Discrimination-Aware Classification},
  year    = {2017},
  month   = {06},
  pages   = {120-134},
  volume  = {5},
  doi     = {10.1089/big.2016.0048},
  groups  = {Fairness},
}

@inproceedings{Calmon2017,
 author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Optimized Pre-Processing for Discrimination Prevention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
 volume = {30},
 year = {2017}
}

 
@TechReport{Hardt2016,
  author   = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  title    = {Equality of {Opportunity} in {Supervised} {Learning}},
  year     = {2016},
  month    = oct,
  note     = {arXiv:1610.02413 [cs] type: article},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.},
  doi      = {10.48550/arXiv.1610.02413},
  file     = {:Hardt2016 - Equality of Opportunity in Supervised Learning.pdf:PDF},
  groups   = {Fairness},
  keywords = {Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1610.02413},
}


@InProceedings{Zafar2017,
  author    = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  title     = {{Fairness Constraints: Mechanisms for Fair Classification}},
  year      = {2017},
  editor    = {Singh, Aarti and Zhu, Jerry},
  month     = {04},
  pages     = {962--970},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {54},
  abstract  = {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.},
  groups    = {Fairness},
  pdf       = {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
  url       = {https://proceedings.mlr.press/v54/zafar17a.html},
}

@Article{wachter2017,
  author    = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal   = {Harv. JL \& Tech.},
  title     = {Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  year      = {2017},
  pages     = {841--887},
  volume    = {31},
  groups    = {Overview xAI / Interpretability},
  publisher = {HeinOnline},
}

@Conference{Ghoba,
  author    = {Ghoba, Sama and Colaner, Nathan},
  booktitle = {35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia},
  title     = {Counterfactual Fairness in Mortgage Lending via Matching and Randomization},
  year      = {2021},
  month     = {12},
  groups    = {HMDA Fairness},
}

 
@Article{Singh2022,
  author     = {Singh, Arashdeep and Singh, Jashandeep and Khan, Ariba and Gupta, Amar},
  journal    = {Machine Learning and Knowledge Extraction},
  title      = {Developing a {Novel} {Fair}-{Loan} {Classifier} through a {Multi}-{Sensitive} {Debiasing} {Pipeline}: {DualFair}},
  year       = {2022},
  issn       = {2504-4990},
  month      = mar,
  number     = {1},
  pages      = {240--253},
  volume     = {4},
  abstract   = {Machine learning (ML) models are increasingly being used for high-stake applications that can greatly impact people’s lives. Sometimes, these models can be biased toward certain social groups on the basis of race, gender, or ethnicity. Many prior works have attempted to mitigate this “model discrimination” by updating the training data (pre-processing), altering the model learning process (in-processing), or manipulating the model output (post-processing). However, more work can be done in extending this situation to intersectional fairness, where we consider multiple sensitive parameters (e.g., race) and sensitive options (e.g., black or white), thus allowing for greater real-world usability. Prior work in fairness has also suffered from an accuracy–fairness trade-off that prevents both accuracy and fairness from being high. Moreover, the previous literature has not clearly presented holistic fairness metrics that work with intersectional fairness. In this paper, we address all three of these problems by (a) creating a bias mitigation technique called DualFair and (b) developing a new fairness metric (i.e., AWI, a measure of bias of an algorithm based upon inconsistent counterfactual predictions) that can handle intersectional fairness. Lastly, we test our novel mitigation method using a comprehensive U.S. mortgage lending dataset and show that our classifier, or fair loan predictor, obtains relatively high fairness and accuracy metrics.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/make4010011},
  file       = {:Singh2022 - Developing a Novel Fair Loan Classifier through a Multi Sensitive Debiasing Pipeline_ DualFair.pdf:PDF},
  groups     = {HMDA Fairness},
  keywords   = {machine learning, algorithmic fairness, bias mitigation, mortgage lending, accuracy–fairness trade-off},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Developing a {Novel} {Fair}-{Loan} {Classifier} through a {Multi}-{Sensitive} {Debiasing} {Pipeline}},
  url        = {https://www.mdpi.com/2504-4990/4/1/11},
  urldate    = {2024-03-19},
}

@inproceedings{Bogen2020,
  author = {Bogen, Miranda and Rieke, Aaron and Ahmed, Shazeda},
  title = {Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination},
  year = {2020},
  isbn = {9781450369367},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3351095.3372877},
  doi = {10.1145/3351095.3372877},
  abstract = {Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {492--500},
  numpages = {9},
  location = {Barcelona, Spain},
  series = {FAT* '20}
}

 
@Article{Islam2023,
  author     = {Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei and Sarwate, Anand D. and Foulds, James R.},
  journal    = {Entropy},
  title      = {Differential {Fairness}: {An} {Intersectional} {Framework} for {Fair} {AI}},
  year       = {2023},
  issn       = {1099-4300},
  month      = apr,
  number     = {4},
  pages      = {660},
  volume     = {25},
  abstract   = {We propose definitions of fairness in machine learning and artificial intelligence systems that are informed by the framework of intersectionality, a critical lens from the legal, social science, and humanities literature which analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including gender, race, sexual orientation, class, and disability. We show that our criteria behave sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. Our theoretical results show that our criteria meaningfully operationalize AI fairness in terms of real-world harms, making the measurements interpretable in a manner analogous to differential privacy. We provide a simple learning algorithm using deterministic gradient methods, which respects our intersectional fairness criteria. The measurement of fairness becomes statistically challenging in the minibatch setting due to data sparsity, which increases rapidly in the number of protected attributes and in the values per protected attribute. To address this, we further develop a practical learning algorithm using stochastic gradient methods which incorporates stochastic estimation of the intersectional fairness criteria on minibatches to scale up to big data. Case studies on census data, the COMPAS criminal recidivism dataset, the HHP hospitalization data, and a loan application dataset from HMDA demonstrate the utility of our methods.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/e25040660},
  file       = {:Islam2023 - Differential Fairness_ an Intersectional Framework for Fair AI.pdf:PDF},
  keywords   = {fairness in AI, AI and society, intersectionality, 80\% rule, privacy},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Differential {Fairness}},
  url        = {https://www.mdpi.com/1099-4300/25/4/660},
  urldate    = {2024-03-19},
}

 
@TechReport{Nam2022,
  author     = {Nam, Tong-yob and Yun, Hayong},
  title      = {Algorithms and {Fairness} in {Lending} {Markets}: {When} {Do} {Humans} and {Machines} {Disagree}?},
  year       = {2022},
  address    = {Rochester, NY},
  month      = oct,
  number     = {4261484},
  abstract   = {This paper uses a machine-learning-based framework to analyze lending decisions. We develop an adversarial learning model that learns the mortgage lending rules for non-minority borrowers and decompose loan decisions into model explained, favorable disagreement, and unfavorable disagreement segments by comparing lenders’ decisions against model predictions. We apply this approach to HMDA data and illustrate various ways to explore differences across different applicant groups, such as the presence of asymmetric disagreement of loan decisions with ML predictions, linking to ex-ante applicant attributes and to ex-post loan performances. The decomposed loan decisions can further uncover economically meaningful information beyond the aggregated counterpart.},
  doi        = {10.2139/ssrn.4261484},
  file       = {Full Text PDF:https\://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID4261484_code410573.pdf?abstractid=4261484&mirid=1:application/pdf},
  groups     = {HMDA Fairness},
  keywords   = {fair lending, machine learning, anomaly detection, generative adversarial network.},
  language   = {en},
  shorttitle = {Algorithms and {Fairness} in {Lending} {Markets}},
  url        = {https://papers.ssrn.com/abstract=4261484},
  urldate    = {2024-03-20},
}

@InProceedings{So2022,
  author    = {So, Wonyoung and Lohia, Pranay and Pimplikar, Rakesh and Hosoi, A.E. and D'Ignazio, Catherine},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  title     = {Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US},
  year      = {2022},
  address   = {New York, NY, USA},
  pages     = {988–1004},
  publisher = {Association for Computing Machinery},
  series    = {FAccT '22},
  abstract  = {Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.},
  doi       = {10.1145/3531146.3533160},
  groups    = {HMDA Fairness},
  isbn      = {9781450393522},
  keywords  = {reparations, racial wealth gap, mortgage lending, housing, fairness},
  location  = {Seoul, Republic of Korea},
  numpages  = {17},
  url       = {https://doi.org/10.1145/3531146.3533160},
}

 
@TechReport{Sharma2022,
  author     = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
  title      = {{FEAMOE}: {Fair}, {Explainable} and {Adaptive} {Mixture} of {Experts}},
  year       = {2022},
  month      = oct,
  note       = {arXiv:2210.04995 [cs] type: article},
  abstract   = {Three key properties that are desired of trustworthy machine learning models deployed in high-stakes environments are fairness, explainability, and an ability to account for various kinds of "drift". While drifts in model accuracy, for example due to covariate shift, have been widely investigated, drifts in fairness metrics over time remain largely unexplored. In this paper, we propose FEAMOE, a novel "mixture-of-experts" inspired framework aimed at learning fairer, more explainable/interpretable models that can also rapidly adjust to drifts in both the accuracy and the fairness of a classifier. We illustrate our framework for three popular fairness measures and demonstrate how drift can be handled with respect to these fairness constraints. Experiments on multiple datasets show that our framework as applied to a mixture of linear experts is able to perform comparably to neural networks in terms of accuracy while producing fairer models. We then use the large-scale HMDA dataset and show that while various models trained on HMDA demonstrate drift with respect to both accuracy and fairness, FEAMOE can ably handle these drifts with respect to all the considered fairness measures and maintain model accuracy as well. We also prove that the proposed framework allows for producing fast Shapley value explanations, which makes computationally efficient feature attribution based explanations of model decisions readily available via FEAMOE.},
  doi        = {10.48550/arXiv.2210.04995},
  file       = {:Sharma2022 - FEAMOE_ Fair, Explainable and Adaptive Mixture of Experts.pdf:PDF},
  groups     = {HMDA Fairness},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
  school     = {arXiv},
  shorttitle = {{FEAMOE}},
  url        = {http://arxiv.org/abs/2210.04995},
  urldate    = {2024-03-20},
}

 
@Article{Faber2013,
  author    = {Faber, Jacob W.},
  journal   = {Housing Policy Debate},
  title     = {Racial {Dynamics} of {Subprime} {Mortgage} {Lending} at the {Peak}},
  year      = {2013},
  issn      = {1051-1482},
  month     = apr,
  number    = {2},
  pages     = {328--349},
  volume    = {23},
  abstract  = {Subprime mortgage lending in the early 2000s was a leading cause of the Great Recession. From 2003 to 2006, subprime loans jumped from 7.6\% of the mortgage market to 20.1\%, with black and Latino borrowers receiving a disproportionate share. This article leveraged the Home Mortgage Disclosure Act data and multinomial regression to model home-purchase mortgage lending in 2006, the peak of the housing boom. The findings expose a complicated story of race and income. Consistent with previous research, blacks and Latinos were more likely and Asians less likely to receive subprime loans than whites were. Income was positively associated with receipt of subprime loans for minorities, whereas the opposite was true for whites. When expensive (jumbo) loans were excluded from the sample, regressions found an even stronger, positive association between income and subprime likelihood for minorities, supporting the theory that wealthier minorities were targeted for subprime loans when they could have qualified for prime loans. This finding also provides another example of an aspect of American life in which minorities are unable to leverage higher class position in the same way as whites are. Contrary to previous research, model estimates did not find that borrowers paid a penalty (in increased likelihood of subprime outcome) for buying homes in minority communities.},
  doi       = {10.1080/10511482.2013.771788},
  file      = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/10511482.2013.771788:application/pdf},
  groups    = {HMDA Fairness},
  keywords  = {homeownership, minorities, mortgages, predatory lending, segregation},
  publisher = {Routledge},
  url       = {https://doi.org/10.1080/10511482.2013.771788},
  urldate   = {2024-03-21},
}

 
@Article{Rugh2015,
  author     = {Rugh, Jacob S. and Albright, Len and Massey, Douglas S.},
  journal    = {Social Problems},
  title      = {Race, {Space}, and {Cumulative} {Disadvantage}: {A} {Case} {Study} of the {Subprime} {Lending} {Collapse}},
  year       = {2015},
  issn       = {0037-7791},
  month      = may,
  number     = {2},
  pages      = {186--218},
  volume     = {62},
  abstract   = {In this article, we describe how residential segregation and individual racial disparities generate racialized patterns of subprime lending and lead to financial loss among black borrowers in segregated cities. We conceptualize race as a cumulative disadvantage because of its direct and indirect effects on socioeconomic status at the individual and neighborhood levels, with consequences that reverberate across a borrower’s life and between generations. Using Baltimore, Maryland as a case study setting, we combine data from reports filed under the Home Mortgage Disclosure Act with additional loan-level data from mortgage-backed securities. We find that race and neighborhood racial segregation are critical factors explaining black disadvantage across successive stages in the process of lending and foreclosure, controlling for differences in borrower credit scores, income, occupancy status, and loan-to-value ratios. We analyze the cumulative cost of predatory lending to black borrowers in terms of reduced disposable income and lost wealth. We find the cost to be substantial. Black borrowers paid an estimated additional 5 to 11 percent in monthly payments and those that completed foreclosure in the sample lost an excess of \$2 million in home equity. These costs were magnified in mostly black neighborhoods and in turn heavily concentrated in communities of color. By elucidating the mechanisms that link black segregation to discrimination we demonstrate how processes of cumulative disadvantage continue to undermine black socioeconomic status in the United States today.En este artículo describimos cómo la segregación residencial y las desigualdades raciales individuales generan patrones raciales en la concesión y la ejecución de préstamos de alto riesgo entre los prestatarios negros en ciudades segregadas. Se presenta el concepto de raza como una desventaja acumulativa ya que tiene efectos económicos directos e indirectos a nivel individual y de barrio, y consecuencias que repercuten de por vida en el prestatario y sus descendientes. Se presenta el estudio de caso de la ciudad de Baltimore, en el estado de Maryland, con la utilización de los informes accesibles gracias a la Ley de Divulgación de Hipotecas y los datos de los instrumentos financieros derivados (CDO). Los resultados señalan que la raza y la segregación racial residencial son factores críticos que explican la desventaja de los afroamericanos en el proceso de préstamo y de ejecución hipotecaria, aún cuando controlamos las diferencias en las puntuaciones de crédito del prestatario, los ingresos, su ocupación y la proporción de valor del préstamo. También analizamos el costo acumulativo de los préstamos de alto riesgo en la reducción de ingreso disponible y en la perdida de riqueza de los prestatarios afroamericanos. Descubrimos que el costo acumulativo es sustancial. Los prestatarios negros pagaron entre el 5 al 11 por ciento más en pagos mensuales y aquellos que llegaron a la ejecución hipotecaria perdieron un exceso de \$2 millones en equidad. Estos costos son más altos en barrios de mayoría afroamericana y en comunidades de color. Para explicar los mecanismos que enlazan la segregación afroamericana y la discriminación mostramos cómo, hoy en día, los procesos de desventaja acumulativa por razón racial siguen socavando la situación socioeconómica de la población negra en los Estados Unidos.spv002\_Rugh5476156636001},
  doi        = {10.1093/socpro/spv002},
  groups     = {HMDA Fairness},
  shorttitle = {Race, {Space}, and {Cumulative} {Disadvantage}},
  url        = {https://doi.org/10.1093/socpro/spv002},
  urldate    = {2024-03-21},
}

@article{2018aequitas,
     title={Aequitas: A Bias and Fairness Audit Toolkit},
     author={Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid}, journal={arXiv preprint arXiv:1811.05577}, year={2018}}

 
@InProceedings{Calders2009,
  author    = {Calders, Toon and Kamiran, Faisal and Pechenizkiy, Mykola},
  booktitle = {2009 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops}},
  title     = {Building {Classifiers} with {Independency} {Constraints}},
  year      = {2009},
  month     = dec,
  note      = {ISSN: 2375-9259},
  pages     = {13--18},
  abstract  = {In this paper we study the problem of classifier learning where the input data contains unjustified dependencies between some data attributes and the class label. Such cases arise for example when the training data is collected from different sources with different labeling criteria or when the data is generated by a biased decision process. When a classifier is trained directly on such data, these undesirable dependencies will carry over to the classifier's predictions. In order to tackle this problem, we study the classification with independency constraints problem: find an accurate model for which the predictions are independent from a given binary attribute. We propose two solutions for this problem and present an empirical validation.},
  doi       = {10.1109/ICDMW.2009.83},
  issn      = {2375-9259},
  keywords  = {Training data, Data mining, Predictive models, Conferences, Electronic mail, Labeling, Prediction algorithms, Constraint optimization, Machine learning, Machine learning algorithms},
  url       = {https://ieeexplore.ieee.org/document/5360534},
  urldate   = {2024-03-23},
}

@InProceedings{Cooper2024,
  author = {Cooper, A. and Lee, Katherine and Choksi, Madiha and Barocas, Solon and De Sa, Christopher and Grimmelmann, James and Kleinberg, Jon and Sen, Siddhartha and Zhang, Baobao},
  title  = {Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification},
  year   = {2024},
  month  = {02},
  groups = {HMDA Fairness},
}

@Conference{Hodges2024,
  author       = {Hope Hodges. and Carolyn Garrity. and James Pope.},
  booktitle    = {Proceedings of the 13th International Conference on Pattern Recognition Applications and Methods - ICPRAM},
  title        = {Deep Learning, Feature Selection and Model Bias with Home Mortgage Loan Classification},
  year         = {2024},
  organization = {INSTICC},
  pages        = {248--255},
  publisher    = {SciTePress},
  doi          = {10.5220/0012326800003654},
  groups       = {HMDA Fairness},
  isbn         = {978-989-758-684-2},
  issn         = {2184-4313},
}

@InProceedings{Ribeiro2016,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title     = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {1135--1144},
  publisher = {Association for Computing Machinery},
  series    = {KDD '16},
  abstract  = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  doi       = {10.1145/2939672.2939778},
  groups    = {Overview xAI / Interpretability},
  isbn      = {9781450342322},
  keywords  = {interpretable machine learning, interpretability, explaining machine learning, black box classifier},
  location  = {San Francisco, California, USA},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2939672.2939778},
}

@InProceedings{Lundberg2017,
  author = {Lundberg, Scott and Lee, Su-In},
  title  = {A Unified Approach to Interpreting Model Predictions},
  year   = {2017},
  month  = {12},
  groups = {Overview xAI / Interpretability},
}

@inproceedings{Karim2023,
  author = {Karim, Rezaul and Shajalal, Md and Graß, Alexander and Döhmen, Till and Chala, Sisay and Boden, Alexander and Beecks, Christian and Decker, Stefan},
  year = {2023},
  month = {10},
  pages = {1-10},
  title = {Interpreting Black-box Machine Learning Models for High Dimensional Datasets},
  doi = {10.1109/DSAA60987.2023.10302562}
}

@Conference{Kusner2017,
  author    = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  booktitle = {31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA},
  title     = {Counterfactual Fairness},
  year      = {2017},
  month     = {03},
  groups    = {Other},
}

@InProceedings{Kearns2019,
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei},
  title  = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
  year   = {2019},
  month  = {01},
  pages  = {100-109},
  doi    = {10.1145/3287560.3287592},
  groups = {Other},
}

@inproceedings{Pleiss2017,
  author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {On Fairness and Calibration},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
  volume = {30},
  year = {2017}
}

@Article{Krishna2022,
  author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Hima},
  year = {2022},
  month = {02},
  pages = {},
  title = {The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective},
  doi = {10.21203/rs.3.rs-2963888/v1}
}

@article{Weerts2023,
  title={Fairlearn: Assessing and Improving Fairness of AI Systems},
  author={Weerts, Hilde and Dudík, Miroslav and Edgar, Richard and Jalali, Adrin and Lutz, Roman and Madaio, Michael},
  journal={J. Mach. Learn. Res.},
  year={2023},
  volume={24},
  pages={257:1-257:8},
  url={https://api.semanticscholar.org/CorpusID:257804972}
}

@Misc{HMDA2022,
  month    = sep,
  title    = {Home {Mortgage} {Disclosure} {Act} ({HMDA}) {Data}},
  year     = {2022},
  abstract = {Learn more about mortgage activity by reviewing HMDA data or download the data for your own analysis.},
  groups   = {Other},
  journal  = {Consumer Financial Protection Bureau},
  language = {en},
  url      = {https://www.consumerfinance.gov/data-research/hmda/},
  urldate  = {2024-04-07},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Overview xAI / Interpretability\;0\;0\;0x0000ffff\;\;Files providing overviews of different interpretability / explainability methods\;;
1 StaticGroup:Other\;0\;0\;0xff008fff\;\;Theoretical Frameworks etc.\;;
1 StaticGroup:Fairness\;0\;1\;0x008000ff\;\;\;;
1 StaticGroup:Benchmarking\;0\;1\;0xff00ffff\;\;\;;
1 StaticGroup:Credit / Mortgage\;0\;1\;0xffff00ff\;\;\;;
1 StaticGroup:HMDA Fairness\;0\;1\;\;\;\;;
}
